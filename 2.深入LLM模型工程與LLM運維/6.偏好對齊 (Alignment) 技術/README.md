偏好對齊 (Alignment) 技術

6.1 RLHF 基本概念：人類回饋、獎勵模型訓練、PPO 演算法
6.2 DPO (Direct Preference Optimization) 與其他替代方案
6.3 偏好資料集的建立與收集困難點
6.4 實務上如何進行 RLHF 微調 (StackLLaMA範例)