## 推論優化 (Inference Optimization)

### 6.1 加速推論的技術：量化、Flash Attention、KV Cache、Speculative Decoding  
### 6.2 工具與框架：vLLM、Text Generation Inference (TGI)、CTranslate2  
### 6.3 實作示例：
- (程式碼) 使用 ExLlama 或 QLoRA 量化模型並比較記憶體消耗與推論速度  
- (程式碼) 測試KV Cache對回應時間的影響