{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark_Basics.ipynb\n",
    "\n",
    "## 簡介 (Introduction)\n",
    "\n",
    "本 Notebook 將示範使用 **PySpark** 進行資料讀取、基本轉換以及簡單分析的操作。透過此範例，您將學習如何：\n",
    "\n",
    "- 初始化 SparkSession（PySpark 的入口點）\n",
    "- 載入資料（CSV、JSON）\n",
    "- 利用 Pandas DataFrame 與 Spark DataFrame 互相轉換\n",
    "- 對資料進行基本的轉換與清理（選擇欄位、篩選、建立衍生欄位、分組聚合）\n",
    "- 使用 Spark 的 DataFrame API 進行簡單分析\n",
    "- 將分析結果儲存回檔案系統\n",
    "- 展示一些基本的性能優化手法（例如快取）\n",
    "\n",
    "請確保您已安裝好 PySpark。\n",
    "若未安裝，可透過：\n",
    "```bash\n",
    "!pip install pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 初始化 SparkSession 與環境設定"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, max, min, mean, stddev, expr, upper\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 建立 SparkSession\n",
    "# 可加入額外設定，例如 offHeap memory 設定\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_Basics_Demo\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 資料讀取\n",
    "\n",
    "此處將示範如何從 CSV 與 JSON 檔案讀取資料，以及從 Pandas DataFrame 建立 Spark DataFrame。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 讀取 CSV 檔案"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 假設有一個 sample_data.csv 檔案\n",
    "# 若無此檔案，可執行以下程式碼建立範例檔案\n",
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(\"id,name,age,city\\n\")\n",
    "    f.write(\"1,Alice,25,New York\\n\")\n",
    "    f.write(\"2,Bob,30,Los Angeles\\n\")\n",
    "    f.write(\"3,Charlie,35,Chicago\\n\")\n",
    "    f.write(\"4,David,40,New York\\n\")\n",
    "    f.write(\"5,Alice,28,Chicago\")"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 讀取 CSV 檔案\n",
    "df_csv = spark.read.csv(\"sample_data.csv\", header=True, inferSchema=True)\n",
    "df_csv.show()\n",
    "df_csv.printSchema()"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 讀取 JSON 檔案\n",
    "此示例假設有一個 `example.json`。若無法檔案可視需要自行建立。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 範例 JSON 檔案 (若無可自行建立)：\n",
    "with open(\"example.json\", \"w\") as f:\n",
    "    f.write('[{\"id\":101, \"item\":\"Book\", \"price\":12.5},\\n')\n",
    "    f.write('{\"id\":102, \"item\":\"Pen\", \"price\":1.5},\\n')\n",
    "    f.write('{\"id\":103, \"item\":\"Laptop\", \"price\":900.0}]')\n",
    "\n",
    "# 讀取 JSON\n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(\"example.json\")\n",
    "df_json.show()\n",
    "df_json.printSchema()"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 從 Pandas DataFrame 轉換為 Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 建立範例 Pandas DataFrame\n",
    "pandas_df = pd.DataFrame({\n",
    "    'id': range(10),\n",
    "    'value': np.random.randn(10),\n",
    "    'category': np.random.choice(['A','B','C'], 10)\n",
    "})\n",
    "\n",
    "# 轉換為 Spark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "spark_df.show()"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 資料轉換與篩選\n",
    "利用 df_csv 進行示範。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 選擇特定欄位\n",
    "df_selected = df_csv.select(\"name\", \"age\")\n",
    "df_selected.show()\n",
    "\n",
    "# 篩選 age > 30\n",
    "df_filtered = df_csv.filter(col(\"age\") > 30)\n",
    "df_filtered.show()\n",
    "\n",
    "# 建立新欄位 age_plus_one = age + 1\n",
    "df_newcol = df_csv.withColumn(\"age_plus_one\", col(\"age\") + 1)\n",
    "df_newcol.show()"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 分組聚合與簡單分析\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 根據 city 分組，計算該 city 的人數\n",
    "df_count_by_city = df_csv.groupBy(\"city\").count()\n",
    "df_count_by_city.show()\n",
    "\n",
    "# 根據 name 分組，計算平均年齡\n",
    "df_avg_age_by_name = df_csv.groupBy(\"name\").agg(avg(\"age\").alias(\"avg_age\"))\n",
    "df_avg_age_by_name.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可嘗試更複雜的聚合，如計算 stddev 或 median（使用 approximate 方法）："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 計算每個城市的年齡統計\n",
    "df_city_stats = df_csv.groupBy(\"city\").agg(\n",
    "    count(\"id\").alias(\"count_id\"),\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    stddev(\"age\").alias(\"std_age\"),\n",
    "    expr(\"percentile_approx(age, 0.5)\").alias(\"median_age\")\n",
    ")\n",
    "df_city_stats.show()"
   ],
   "execution_count": null,
   "outputs": []
  },    
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 使用 SQL 查詢\n",
    "\n",
    "可以將 DataFrame 註冊為臨時視圖，然後使用 SQL 語法對其進行查詢。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_csv.createOrReplaceTempView(\"people\")\n",
    "sql_result = spark.sql(\"SELECT city, AVG(age) as avg_age FROM people GROUP BY city ORDER BY avg_age DESC\")\n",
    "sql_result.show()"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 資料輸出\n",
    "將結果輸出為 CSV 或 Parquet 等格式。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 輸出為 CSV\n",
    "df_avg_age_by_name.coalesce(1).write.mode('overwrite').option('header','true').csv('output_csv')\n",
    "\n",
    "# 輸出為 Parquet\n",
    "df_city_stats.write.mode('overwrite').parquet('output_parquet')"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 與 Pandas DataFrame 互轉\n",
    "\n",
    "可將 Spark DataFrame 轉為 Pandas DataFrame，以進行本地分析或視覺化。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pandas_result = df_avg_age_by_name.toPandas()\n",
    "pandas_result"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 性能優化示例\n",
    "\n",
    "### 8.1 快取 DataFrame\n",
    "在反覆使用同一 DataFrame 時，可以透過 `cache()` 來快取資料以加速後續動作。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 將 df_csv 快取\n",
    "df_csv.cache()\n",
    "# 執行動作，觸發快取\n",
    "df_csv.count()"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 分區調整\n",
    "可透過 `repartition()` 改變分區數量，以平衡工作負載。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_repart = df_csv.repartition(4)\n",
    "print(\"分區數：\", df_repart.rdd.getNumPartitions())"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 結束 SparkSession\n",
    "\n",
    "在所有操作完成後，呼叫 `spark.stop()` 以釋放資源。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "spark.stop()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "本 Notebook 展示了：\n",
    "- 建立 SparkSession\n",
    "- 從 CSV、JSON 載入資料，從 Pandas DF 轉為 Spark DF\n",
    "- 資料的選擇、篩選、衍生欄位新增\n",
    "- 分組聚合與統計分析\n",
    "- 使用 Spark SQL 查詢資料\n",
    "- 將處理結果輸出成檔案\n",
    "- 簡單的快取與分區操作優化性能\n",
    "\n",
    "透過這些步驟，你應該已對 PySpark 的基本用法有初步了解，可作為後續更進階大數據處理、機器學習與分析的基石。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
