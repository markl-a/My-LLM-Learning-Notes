{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分散式環境數據處理流水線示範\n",
    "\n",
    "本教學展示如何在分散式環境中建立完整的數據處理流水線，包含：\n",
    "1. 環境設置\n",
    "2. 資料擷取\n",
    "3. 資料清理和預處理\n",
    "4. 特徵工程\n",
    "5. 模型訓練\n",
    "6. 模型評估\n",
    "7. 預測和結果儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 導入所需套件\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 創建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataPipelineDemo\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"10g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 設定記錄層級\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 資料擷取\n",
    "### 2.1 從多個來源讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 定義資料schema\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"income\", DoubleType(), True),\n",
    "    StructField(\"registration_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"transaction_date\", DateType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"product_category\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 讀取客戶資料\n",
    "customers_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(customer_schema) \\\n",
    "    .csv(\"data/customers.csv\")\n",
    "\n",
    "# 讀取交易資料\n",
    "transactions_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(transaction_schema) \\\n",
    "    .csv(\"data/transactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 資料初步檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 顯示資料基本信息\n",
    "print(\"客戶資料概覽：\")\n",
    "customers_df.printSchema()\n",
    "customers_df.show(5)\n",
    "\n",
    "print(\"\\n交易資料概覽：\")\n",
    "transactions_df.printSchema()\n",
    "transactions_df.show(5)\n",
    "\n",
    "# 檢查資料筆數\n",
    "print(\"\\n資料筆數統計：\")\n",
    "print(f\"客戶資料筆數: {customers_df.count()}\")\n",
    "print(f\"交易資料筆數: {transactions_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 資料清理和預處理\n",
    "### 3.1 處理缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 檢查缺失值\n",
    "def check_null_values(df, df_name):\n",
    "    print(f\"\\n{df_name} 缺失值統計：\")\n",
    "    df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "check_null_values(customers_df, \"客戶資料\")\n",
    "check_null_values(transactions_df, \"交易資料\")\n",
    "\n",
    "# 處理缺失值\n",
    "customers_cleaned = customers_df \\\n",
    "    .na.fill({\"age\": customers_df.select(avg(\"age\")).first()[0]}) \\\n",
    "    .na.fill({\"income\": customers_df.select(avg(\"income\")).first()[0]}) \\\n",
    "    .na.fill({\"gender\": \"unknown\"})\n",
    "\n",
    "transactions_cleaned = transactions_df \\\n",
    "    .na.drop()  # 交易資料中的缺失值直接刪除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 異常值處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 計算數值欄位的統計量用於異常值檢測\n",
    "def detect_outliers(df, numeric_cols):\n",
    "    for col_name in numeric_cols:\n",
    "        quantiles = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "        iqr = quantiles[1] - quantiles[0]\n",
    "        lower_bound = quantiles[0] - 1.5 * iqr\n",
    "        upper_bound = quantiles[1] + 1.5 * iqr\n",
    "        \n",
    "        print(f\"\\n{col_name} 的異常值界限：\")\n",
    "        print(f\"Lower bound: {lower_bound}\")\n",
    "        print(f\"Upper bound: {upper_bound}\")\n",
    "        \n",
    "        # 過濾異常值\n",
    "        df = df.filter((col(col_name) >= lower_bound) & \n",
    "                      (col(col_name) <= upper_bound))\n",
    "    return df\n",
    "\n",
    "# 處理客戶資料中的異常值\n",
    "customers_cleaned = detect_outliers(customers_cleaned, [\"age\", \"income\"])\n",
    "transactions_cleaned = detect_outliers(transactions_cleaned, [\"amount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 特徵工程\n",
    "### 4.1 特徵創建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 為客戶創建統計特徵\n",
    "customer_features = transactions_cleaned \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        count(\"transaction_id\").alias(\"transaction_count\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction_amount\"),\n",
    "        sum(\"amount\").alias(\"total_spend\"),\n",
    "        stddev(\"amount\").alias(\"transaction_amount_std\"),\n",
    "        datediff(max(\"transaction_date\"), min(\"transaction_date\")).alias(\"customer_lifetime_days\")\n",
    "    )\n",
    "\n",
    "# 計算每個產品類別的消費比例\n",
    "category_pivot = transactions_cleaned \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .pivot(\"product_category\") \\\n",
    "    .agg(sum(\"amount\")) \\\n",
    "    .fillna(0)\n",
    "\n",
    "# 合併所有特徵\n",
    "final_features = customers_cleaned \\\n",
    "    .join(customer_features, \"customer_id\") \\\n",
    "    .join(category_pivot, \"customer_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 特徵轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 創建特徵處理流水線\n",
    "categorical_cols = [\"gender\"]\n",
    "numeric_cols = [col_name for col_name in final_features.columns \n",
    "               if col_name not in [\"customer_id\", \"gender\", \"registration_date\"]]\n",
    "\n",
    "# 字串索引轉換器\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\") \n",
    "           for col in categorical_cols]\n",
    "\n",
    "# 特徵向量組合器\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[col+\"_idx\" for col in categorical_cols] + numeric_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# 標準化處理器\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型訓練\n",
    "### 5.1 準備訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 創建目標變數（示例：高價值客戶分類）\n",
    "final_features = final_features \\\n",
    "    .withColumn(\"label\", \n",
    "                when(col(\"total_spend\") > final_features.select(\n",
    "                    percentile_approx(\"total_spend\", 0.8)).first()[0], 1).otherwise(0))\n",
    "\n",
    "# 分割訓練和測試資料\n",
    "train_data, test_data = final_features.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 建立和訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 創建隨機森林分類器\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "                          featuresCol=\"scaled_features\",\n",
    "                          numTrees=100)\n",
    "\n",
    "# 建立完整的流水線\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler, rf])\n",
    "\n",
    "# 訓練模型\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型評估"
   ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
     "# 在測試集上進行預測\n",
     "predictions = model.transform(test_data)\n",
     "\n",
     "# 評估模型效能\n",
     "evaluator = MulticlassClassificationEvaluator(\n",
     "    labelCol=\"label\", \n",
     "    predictionCol=\"prediction\", \n",
     "    metricName=\"accuracy\"\n",
     ")\n",
     "\n",
     "accuracy = evaluator.evaluate(predictions)\n",
     "print(f\"模型準確率: {accuracy}\")\n",
     "\n",
     "# 計算其他評估指標\n",
     "evaluator.setMetricName(\"weightedPrecision\")\n",
     "precision = evaluator.evaluate(predictions)\n",
     "print(f\"加權精確率: {precision}\")\n",
     "\n",
     "evaluator.setMetricName(\"weightedRecall\")\n",
     "recall = evaluator.evaluate(predictions)\n",
     "print(f\"加權召回率: {recall}\")"
    ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 檢視詳細預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 顯示預測結果範例\n",
    "predictions.select(\"customer_id\", \"label\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# 計算混淆矩陣\n",
    "confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "print(\"混淆矩陣:\")\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 預測和結果儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 創建預測函數\n",
    "def predict_new_data(new_data, model):\n",
    "    predictions = model.transform(new_data)\n",
    "    return predictions.select(\"customer_id\", \"prediction\", \"probability\")\n",
    "\n",
    "# 儲存模型\n",
    "model_path = f\"models/customer_classification_{datetime.now().strftime('%Y%m%d')}\"\n",
    "model.write().overwrite().save(model_path)\n",
    "print(f\"模型已儲存至: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 儲存預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 儲存預測結果\n",
    "predictions.select(\"customer_id\", \"prediction\", \"probability\") \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"output/predictions\")\n",
    "\n",
    "print(\"預測結果已儲存至: output/predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 模型部署準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 創建模型部署所需的元數據\n",
    "model_metadata = {\n",
    "    \"model_version\": datetime.now().strftime(\"%Y%m%d\"),\n",
    "    \"features\": {\n",
    "        \"categorical\": categorical_cols,\n",
    "        \"numerical\": numeric_cols\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"模型元數據：\")\n",
    "print(model_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 資源清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 清理 Spark Session\n",
    "spark.stop()\n",
    "print(\"資源已清理完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 總結與建議\n",
    "\n",
    "本示範展示了完整的分散式數據處理流水線，包括：\n",
    "1. 資料擷取和整合\n",
    "2. 資料清理和預處理\n",
    "3. 特徵工程\n",
    "4. 模型訓練和評估\n",
    "5. 結果儲存和部署準備\n",
    "\n",
    "建議在實際應用時：\n",
    "- 根據實際數據量調整 Spark 配置\n",
    "- 增加更多的數據質量檢查\n",
    "- 實作更完整的錯誤處理機制\n",
    "- 添加詳細的日誌記錄\n",
    "- 建立自動化測試流程"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}