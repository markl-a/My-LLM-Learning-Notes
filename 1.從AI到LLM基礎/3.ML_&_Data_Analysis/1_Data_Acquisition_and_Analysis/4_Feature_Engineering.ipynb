{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徵工程 (Feature Engineering) 完整指南\n",
    "\n",
    "本 Notebook 將示範完整的特徵工程流程與技巧，包括：\n",
    "1. 特徵選擇 (Feature Selection)\n",
    "2. 特徵縮放 (Feature Scaling)\n",
    "3. 特徵組合 (Feature Combination)\n",
    "4. 類別特徵編碼 (Categorical Encoding)\n",
    "5. 文本特徵處理 (Text Feature Processing)\n",
    "6. 時間特徵處理 (Time Feature Engineering)\n",
    "7. Embeddings 簡介\n",
    "8. PCA 降維分析 (Dimensionality Reduction via PCA)\n",
    "9. Pipeline 實務整合\n",
    "\n",
    "我們將以 `iris` 鳶尾花資料集為基礎，並添加一些人造特徵（類別、日期、文本）以示範多種特徵工程方法。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE, VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import category_encoders as ce\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入與準備資料\n",
    "\n",
    "使用 iris 資料集，並添加人造類別特徵、日期特徵、文本特徵，以展示各種特徵工程方法。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 載入iris資料集\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "df['target'] = pd.Categorical.from_codes(iris['target'], categories=iris['target_names'])\n",
    "\n",
    "# 添加類別特徵\n",
    "df['color'] = np.random.choice(['Red','Green','Blue'], size=len(df))\n",
    "df['region'] = np.random.choice(['North','South','East','West'], size=len(df))\n",
    "\n",
    "# 添加日期特徵\n",
    "dates = pd.date_range(start='2021-01-01', periods=len(df))\n",
    "np.random.shuffle(dates.values)\n",
    "df['date'] = dates\n",
    "\n",
    "# 添加文本特徵\n",
    "feedback_choices = ['產品品質優良', '服務態度良好', '價格合理', '出貨速度快',\n",
    "                   '產品有瑕疵', '服務需改善', '價格偏高', '出貨太慢']\n",
    "df['feedback'] = np.random.choice(feedback_choices, size=len(df))\n",
    "\n",
    "# 添加常數特徵 (用於示範方差過低特徵移除)\n",
    "df['constant_feature'] = 1.0\n",
    "\n",
    "df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料切分\n",
    "\n",
    "將資料分成訓練集與測試集，目標欄位為 `target`。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 特徵選擇 (Feature Selection)\n",
    "\n",
    "包括：\n",
    "- VarianceThreshold\n",
    "- SelectKBest (ANOVA F-value, Mutual Information)\n",
    "- RFE (Recursive Feature Elimination)\n",
    "- 基於模型的特徵重要度 (RandomForest)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 選擇數值特徵進行特徵選擇示範\n",
    "X_train_num = X_train.select_dtypes(include=[np.number])\n",
    "y_train_num = y_train\n",
    "\n",
    "# 1. VarianceThreshold: 移除方差過低的特徵\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "X_vt = vt.fit_transform(X_train_num)\n",
    "removed_features = X_train_num.columns[~vt.get_support()]\n",
    "print(\"透過VarianceThreshold移除的特徵：\", removed_features.tolist())"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. SelectKBest (ANOVA F-value)\n",
    "from sklearn.feature_selection import f_classif\n",
    "skb_f = SelectKBest(score_func=f_classif, k=4)\n",
    "X_skb_f = skb_f.fit_transform(X_train_num, y_train_num)\n",
    "selected_features_f = X_train_num.columns[skb_f.get_support()]\n",
    "print(\"ANOVA F-value 選擇出的特徵：\", selected_features_f.tolist())\n",
    "\n",
    "# 也可使用互信息\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "skb_mi = SelectKBest(score_func=mutual_info_classif, k=4)\n",
    "X_skb_mi = skb_mi.fit_transform(X_train_num, y_train_num)\n",
    "selected_features_mi = X_train_num.columns[skb_mi.get_support()]\n",
    "print(\"Mutual Info 選擇出的特徵：\", selected_features_mi.tolist())"
   ],
   "execution_count": null,
   "outputs": []
  },   
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3. RFE (以邏輯回歸為基礎模型)\n",
    "from sklearn.feature_selection import RFE\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "rfe = RFE(estimator=lr, n_features_to_select=4)\n",
    "X_rfe = rfe.fit_transform(X_train_num, y_train_num)\n",
    "selected_features_rfe = X_train_num.columns[rfe.get_support()]\n",
    "print(\"RFE 選擇的特徵：\", selected_features_rfe.tolist())"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 4. 基於模型的特徵重要度 (RandomForest)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_num, y_train_num)\n",
    "importances = rf.feature_importances_\n",
    "imp_df = pd.DataFrame({'feature': X_train_num.columns, 'importance': importances})\n",
    "imp_df = imp_df.sort_values('importance', ascending=False)\n",
    "print(\"RandomForest 特徵重要度：\")\n",
    "print(imp_df)"
   ],
   "execution_count": null,
   "outputs": []
  },   
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特徵縮放 (Feature Scaling)\n",
    "\n",
    "比較 StandardScaler, MinMaxScaler, RobustScaler 對分佈的影響。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numeric_cols = X_train_num.columns\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 5*len(numeric_cols)))\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    data = X_train_num[col].values.reshape(-1, 1)\n",
    "    plt.subplot(len(numeric_cols), 4, i*4 + 1)\n",
    "    sns.histplot(data=data, bins=30)\n",
    "    plt.title(f'Original - {col}')\n",
    "    for j, (name, scaler) in enumerate(scalers.items()):\n",
    "        scaled_data = scaler.fit_transform(data)\n",
    "        plt.subplot(len(numeric_cols), 4, i*4 + j + 2)\n",
    "        sns.histplot(data=scaled_data, bins=30)\n",
    "        plt.title(f'{name} - {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },   
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特徵組合 (Feature Combination)\n",
    "\n",
    "示範使用 PolynomialFeatures 產生多項式特徵，以及手動產生一些數值特徵交互作用項。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_train_num)\n",
    "print(\"原始數值特徵數：\", X_train_num.shape[1])\n",
    "print(\"多項式特徵後特徵數：\", X_poly.shape[1])"
   ],
   "execution_count": null,
   "outputs": []
  },   
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可自行創建新特徵，例如收入與年齡的比值或日期提取出年度、月份等特徵。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 時間特徵處理範例\n",
    "X_train['year'] = X_train['date'].dt.year\n",
    "X_train['month'] = X_train['date'].dt.month\n",
    "X_train['dayofweek'] = X_train['date'].dt.dayofweek\n",
    "\n",
    "# 數值特徵互動\n",
    "X_train['income_age_ratio'] = X_train['income'] / (X_train['sepal length (cm)']+0.1)\n", 
    "# 僅示範，實務需確保無除以0情況"
   ],
   "execution_count": null,
   "outputs": []
  },   
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 類別特徵編碼 (Categorical Encoding)\n",
    "\n",
    "示範各種編碼方法：\n",
    "- One-Hot Encoding\n",
    "- Ordinal Encoding\n",
    "- Target Encoding (需外部套件 category_encoders)\n",
    "- Binary Encoding (同樣使用 category_encoders)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cat_cols = ['color','region','categorical_feature'] if 'categorical_feature' in X_train.columns else ['color','region']\n",
    "\n",
    "# One-Hot Encoding\n",
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "X_ohe = ohe.fit_transform(X_train[cat_cols])\n",
    "print(\"One-Hot編碼後形狀：\", X_ohe.shape)\n",
    "\n",
    "# Ordinal Encoding (需先定義類別順序)\n",
    "ord_enc = OrdinalEncoder()\n",
    "X_ord = ord_enc.fit_transform(X_train[cat_cols])\n",
    "print(\"Ordinal編碼後形狀：\", X_ord.shape)\n",
    "\n",
    "# Target Encoding\n",
    "te = ce.TargetEncoder()\n",
    "X_te = te.fit_transform(X_train[cat_cols], y_train)\n",
    "print(\"Target Encoding後形狀：\", X_te.shape)\n",
    "\n",
    "# Binary Encoding\n",
    "be = ce.BinaryEncoder()\n",
    "X_be = be.fit_transform(X_train[cat_cols])\n",
    "print(\"Binary Encoding後形狀：\", X_be.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },   
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 文本特徵處理 (Text Feature Processing)\n",
    "\n",
    "使用 CountVectorizer 或 TfidfVectorizer 對文本特徵進行簡單表示，若有需要可使用更進階的Embedding方法（如 Word2Vec、BERT 等）。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_text = X_train['feedback']\n",
    "cv = CountVectorizer()\n",
    "X_text_cv = cv.fit_transform(X_text)\n",
    "print(\"CountVectorizer 特徵維度：\", X_text_cv.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embeddings 簡介\n",
    "\n",
    "Embeddings 是將類別或詞彙映射到低維向量空間的技術。在NLP中常用 Word2Vec 等工具學習詞向量。此處不實際訓練，但示範如何使用 gensim 對文本分詞後訓練 Word2Vec。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install gensim"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = X_train['feedback'].apply(lambda x: list(x)) # 簡單將字串轉為字元list示範\n",
    "model_w2v = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "word_vec = model_w2v.wv['產品'] # 假設 '產品' 在詞彙中\n",
    "print(\"'產品'的詞向量：\", word_vec)"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "實務上可使用更完善的分詞與語言模型取得更好的嵌入表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA 降維分析\n",
    "\n",
    "對數值特徵進行 PCA，觀察資料在低維空間的分佈。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_num_train = X_train.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_num_train)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1','PC2'])\n",
    "pca_df['target'] = y_train.reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='target', data=pca_df)\n",
    "plt.title('PCA Result')\n",
    "plt.show()\n",
    "\n",
    "print('解釋變異比例：', pca.explained_variance_ratio_)\n",
    "print('累積解釋變異比例：', np.cumsum(pca.explained_variance_ratio_))"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pipeline 實務整合\n",
    "\n",
    "透過 Pipeline 將特徵工程步驟與模型訓練串連，使流程更簡潔與可重現。\n",
    "\n",
    "示範一個簡單的 Pipeline：\n",
    "1. 對數值特徵做 StandardScaler\n",
    "2. 類別特徵做 One-Hot Encoding\n",
    "3. 訓練 Logistic Regression 模型"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns\n",
    "cat_features = ['color','region']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, cat_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('clf', LogisticRegression(max_iter=10000))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))"
   ],
   "execution_count": null,
   "outputs": []
  },  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "本 Notebook 展示了多種特徵工程技巧，包括特徵選擇、特徵縮放、特徵組合、類別編碼、文本特徵處理、時間特徵處理、簡介 Embeddings 概念，以及 PCA 進行降維分析。並透過 Pipeline 將整個流程整合，使得特徵工程與模型訓練可重複執行、易於維護。\n",
    "\n",
    "實務中，請根據資料特性、模型需求及專案目標靈活運用上述方法。特徵工程是一門需要實驗、領域知識與直覺的藝術，唯有不斷嘗試才能找到最佳的特徵處理策略。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3"
  },
  "colab": {
    "name": "Feature_Engineering.ipynb",
    "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

