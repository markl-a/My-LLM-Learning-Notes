{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基於樹的模型 (Tree-Based Models)\n",
    "\n",
    "這個 Notebook 將介紹幾種常用的基於樹的模型，包括：\n",
    "\n",
    "1.  **決策樹 (Decision Tree)**\n",
    "    -   1.1 分類樹\n",
    "    -   1.2 回歸樹\n",
    "2.  **隨機森林 (Random Forest)**\n",
    "3.  **梯度提升樹 (Gradient Boosting Decision Trees, GBDT)**\n",
    "    -   3.1 XGBoost\n",
    "    -   3.2 LightGBM\n",
    "\n",
    "我們將使用 `pandas`、`numpy`、`matplotlib`、`seaborn`、`scikit-learn`、`xgboost` 和 `lightgbm` 等套件來實作這些模型，並使用範例資料進行說明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='decision-tree'></a>\n",
    "## 1. 決策樹 (Decision Tree)\n",
    "\n",
    "決策樹是一種常見的機器學習演算法，可用於**分類**和**迴歸**問題。它通過遞迴地將資料劃分為不同的子集來建構樹狀模型，每個節點代表一個特徵，每個分支代表一個決策規則，每個葉節點代表一個輸出值 (類別標籤或連續值)。\n",
    "\n",
    "**優點:**\n",
    "\n",
    "-   易於理解和解釋 (可視覺化)。\n",
    "-   可以處理數值型和類別型特徵。\n",
    "-   不需要對資料進行特徵縮放。\n",
    "\n",
    "**缺點:**\n",
    "\n",
    "-   容易過度擬合。\n",
    "-   對訓練資料中的小變化敏感。\n",
    "-   可能找到的不是全域最佳解。\n",
    "\n",
    "**建構決策樹的關鍵概念:**\n",
    "\n",
    "-   **特徵選擇:**  在每個節點選擇最佳的特徵來劃分資料。常用的指標包括：\n",
    "    -   **資訊增益 (Information Gain)** (ID3 演算法): 基於熵 (Entropy) 的概念，選擇資訊增益最大的特徵。\n",
    "    -   **增益比率 (Gain Ratio)** (C4.5 演算法): 對資訊增益的改進，考慮了特徵的取值數量。\n",
    "    -   **吉尼係數 (Gini Impurity)** (CART 演算法):  衡量資料的不純度，選擇吉尼係數最小的特徵。\n",
    "-   **停止條件:**  決定何時停止樹的生長，例如：\n",
    "    -   達到最大深度。\n",
    "    -   節點中的樣本數少於某個閾值。\n",
    "    -   節點的不純度低於某個閾值。\n",
    "-   **剪枝 (Pruning):**  減少樹的複雜度，防止過度擬合。常見的剪枝方法包括：\n",
    "    -   **預剪枝 (Pre-pruning):**  在樹的建構過程中，提前停止樹的生長。\n",
    "    -   **後剪枝 (Post-pruning):**  在樹完全生長後，再剪去一些分支。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 分類樹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 scikit-learn 載入鳶尾花資料集\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# 將資料劃分為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 建立決策樹分類器 (使用 Gini 係數)\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42) \n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 評估模型\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 視覺化決策樹\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "plt.show()"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {
     "scrolled": false
    },
    "source": [
    "[Image of Decision Tree]"
    ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 回歸樹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 scikit-learn 載入波士頓房價資料集\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "\n",
    "# 將資料劃分為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 建立決策樹回歸器 (使用 MSE)\n",
    "reg = DecisionTreeRegressor(criterion='squared_error', max_depth=3, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# 評估模型\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared (R^2):\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化決策樹\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(reg, filled=True, feature_names=boston.feature_names)\n",
    "plt.show()"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
    "[Image of Decision Tree Regressor]"
    ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='random-forest'></a>\n",
    "## 2. 隨機森林 (Random Forest)\n",
    "\n",
    "隨機森林是一種**集成學習**方法，它通過建構多個決策樹並將它們的預測結果進行平均 (迴歸) 或投票 (分類) 來進行預測。隨機森林通常比單一決策樹具有更好的泛化能力，並且可以降低過度擬合的風險。\n",
    "\n",
    "**隨機性體現在兩個方面:**\n",
    "\n",
    "-   **樣本隨機:**  從訓練集中隨機抽取樣本 (Bootstrap 抽樣) 來訓練每個決策樹。\n",
    "-   **特徵隨機:**  在每個節點分裂時，從所有特徵中隨機選擇一部分特徵來考慮。\n",
    "\n",
    "**優點:**\n",
    "\n",
    "-   通常具有較高的準確性。\n",
    "-   可以處理大量的特徵。\n",
    "-   可以估計特徵的重要性。\n",
    "-   對異常值不太敏感。\n",
    "\n",
    "**缺點:**\n",
    "\n",
    "-   模型較為複雜，解釋性不如單一決策樹。\n",
    "-   訓練時間可能較長，特別是當樹的數量較多時。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立隨機森林分類器\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=3, random_state=42)\n",
    "\n",
    "# 使用鳶尾花資料集\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# 評估模型\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徵重要性\n",
    "feature_importances = pd.Series(rf_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# 繪製特徵重要性圖表\n",
    "sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features (Random Forest)\")\n",
    "plt.show()"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {
     "scrolled": false
    },
    "source": [
    "[Image of Feature Importances (Random Forest)]"
    ]
   },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立隨機森林迴歸器\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, criterion='squared_error', max_depth=3, random_state=42)\n",
    "\n",
    "# 使用波士頓房價資料集\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = rf_reg.predict(X_test)\n",
    "\n",
    "# 評估模型\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared (R^2):\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gradient-boosting'></a>\n",
    "## 3. 梯度提升樹 (Gradient Boosting Decision Trees, GBDT)\n",
    "\n",
    "梯度提升樹 (GBDT) 也是一種**集成學習**方法，它通過**循序地**建構多個決策樹來進行預測。每個新的決策樹都試圖修正前一個決策樹的預測誤差。GBDT 通常比隨機森林具有更好的準確性，但訓練時間也更長，且更容易過度擬合。\n",
    "\n",
    "**核心思想:**\n",
    "\n",
    "1.  建構一個初始的決策樹 (通常是一個簡單的樹)。\n",
    "2.  計算當前模型的預測誤差 (殘差)。\n",
    "3.  建構一個新的決策樹來預測殘差。\n",
    "4.  將新的決策樹加到現有模型中，並更新預測結果。\n",
    "5.  重複步驟 2-4，直到達到指定的樹的數量或誤差不再下降。\n",
    "\n",
    "**優點:**\n",
    "\n",
    "-   通常具有很高的準確性。\n",
    "-   可以處理數值型和類別型特徵。\n",
    "-   可以估計特徵的重要性。\n",
    "\n",
    "**缺點:**\n",
    "\n",
    "-   訓練時間可能較長，特別是當樹的數量較多時。\n",
    "-   容易過度擬合，需要仔細調整參數。\n",
    "-   模型較為複雜，解釋性不如單一決策樹。\n",
    "\n",
    "### 3.1 XGBoost\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) 是一個高效且流行的 GBDT 實作，它在原始 GBDT 演算法的基礎上進行了多項改進，包括：\n",
    "\n",
    "-   **正則化:**  XGBoost 在目標函數中加入了正則化項，以控制模型的複雜度，防止過度擬合。\n",
    "-   **二階導數:** XGBoost 使用了損失函數的二階導數 (Hessian 矩陣) 來加速訓練過程，並提高模型的準確性。\n",
    "-   **缺失值處理:** XGBoost 可以自動處理缺失值。\n",
    "-   **平行化:** XGBoost 支援平行化訓練，可以加快訓練速度。\n",
    "-   **高效能:** XGBoost 經過了優化，具有很高的執行效率。\n",
    "\n",
    "首先，您需要安裝 XGBoost 套件：\n",
    "\n",
    "```bash\n",
    "pip install xgboost\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 XGBoost 分類器\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# 使用鳶尾花資料集\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# 評估模型\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徵重要性\n",
    "feature_importances = pd.Series(xgb_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# 繪製特徵重要性圖表\n",
    "sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features (XGBoost)\")\n",
    "plt.show()"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
    "[Image of Feature Importances (XGBoost)]"
    ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 XGBoost 迴歸器\n",
    "xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# 使用波士頓房價資料集\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "\n",
    "# 評估模型\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared (R^2):\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LightGBM\n",
    "\n",
    "LightGBM 是一個由微軟開發的 GBDT 框架，它在 XGBoost 的基礎上進行了進一步的優化，主要包括：\n",
    "\n",
    "-   **基於直方圖的演算法:** LightGBM 使用基於直方圖的演算法來尋找最佳的節點分裂點，可以減少記憶體使用量和計算時間。\n",
    "-   **帶有深度限制的葉子生長 (Leaf-wise) 策略:**  與 XGBoost 的按層生長 (Level-wise) 策略不同，LightGBM 採用帶有深度限制的葉子生長策略，可以更快地收斂，並降低過度擬合的風險。\n",
    "-   **特徵並行和資料並行:** LightGBM 支援特徵並行和資料並行，可以進一步加快訓練速度。\n",
    "-   **高效能:** LightGBM 通常比 XGBoost 訓練速度更快，記憶體使用量更低。\n",
    "\n",
    "首先，您需要安裝 LightGBM 套件：\n",
    "\n",
    "```bash\n",
    "pip install lightgbm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 LightGBM 分類器\n",
    "lgb_clf = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# 使用鳶尾花資料集\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = lgb_clf.predict(X_test)\n",
    "\n",
    "# 評估模型\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徵重要性\n",
    "feature_importances = pd.Series(lgb_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# 繪製特徵重要性圖表\n",
    "sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features (LightGBM)\")\n",
    "plt.show()"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
    "[Image of Feature Importances (LightGBM)]"
    ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 LightGBM 迴歸器\n",
    "lgb_reg = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# 使用波士頓房價資料集\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "lgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = lgb_reg.predict(X_test)\n",
    "\n",
    "# 評估模型\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared (R^2):\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "這個 Notebook 介紹了幾種常用的基於樹的模型，包括決策樹、隨機森林和梯度提升樹 (XGBoost、LightGBM)。這些模型在各種機器學習任務中都表現出色，是您機器學習工具箱中不可或缺的一部分。\n",
    "\n",
    "**選擇哪個模型?**\n",
    "\n",
    "-   **決策樹:**  簡單易懂，可作為 baseline 模型，或用於需要模型解釋性的場景。\n",
    "-   **隨機森林:**  通常比決策樹具有更好的準確性和泛化能力，適用於大多數情況。\n",
    "-   **GBDT (XGBoost、LightGBM):**  通常具有最高的準確性，但需要更長的訓練時間和更多的參數調整。XGBoost 更為成熟穩定，LightGBM 則通常更快更輕量。\n",
    "\n",
    "建議您根據具體的應用場景和資料特性來選擇合適的模型，並通過交叉驗證等方法來評估模型的效能，並調整模型的參數以獲得最佳結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考資料\n",
    "\n",
    "-   [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "-   [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "-   [LightGBM Documentation](https://lightgbm.readthedocs.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}