## DL/Dive into Deep learing 個人複習筆記

以下章節程式碼的部分會以 pytorch 以及 tensorflow 2 為主

### 1_引言

### 2_預備知識
- 2.1. 資料操作
- 2.2. 資料預處理
- 2.3. 線性代數
- 2.4. 微積分
- 2.5. 自動微分
- 2.6. 機率
- 2.7. 查閱文件

### 3_線性神經網路
- 3.1. 線性回歸
- 3.2. 從零開始實現線性回歸
- 3.3. 線性回歸的簡潔實現
- 3.4. softmax回歸
- 3.5. 圖像分類資料集
- 3.6. 從零開始實現softmax回歸
- 3.7. softmax回歸的簡潔實現


### 4_多層感知機
- 4.1. 多層感知機
- 4.2. 從零開始實現多層感知機
- 4.3. 多層感知機的簡潔實現
- 4.4. 模型選擇,欠擬和和過擬和
- 4.5. 權重衰減
- 4.6. 暫退法(Dropout)
- 4.7. 前向傳播,反向傳播和計算圖
- 4.8. 數值穩定性和模型初始化
- 4.9. 環境和分布偏移
- 4.10. 實戰Kaggle比賽:預測房價

### 5_深度學習計算
- 5.1. 層和塊
- 5.2. 參數管理
- 5.3. 延後初始化
- 5.4. 自定義層
- 5.5. 讀寫文件
- 5.6. GPU

### 6_卷積神經網路
- 6.1. 從全連接層到卷積
- 6.2. 圖像卷積
- 6.3. 填充和步幅
- 6.4. 多輸入多輸出通道
- 6.5. 匯聚層
- 6.6. 卷積神經網路(LeNet)

### 7_現代卷積神經網路
- 7.1. 深度卷積神經網路(AlexNet)
- 7.2. 使用塊的網路(VGG)
- 7.3. 網路中的網路(NiN)
- 7.4. 含並行連結的網路(GoogleNet)
- 7.5. 批量規範化
- 7.6. 殘差網路(ResNet)
- 7.7. 稠密連接網路(DenseNet)

### 8_循環神經網路
- 8.1. 序列模型
- 8.2. 文本預處理
- 8.3. 語言模型和資料集
- 8.4. 循環神經網路
- 8.5. 從零開始實現循環神經網路
- 8.6. 循環神經網路的簡潔實現
- 8.7. 時間反向傳播(Back propagation through time)

### 9_現代循環神經網路
- 9.1. 門控制循環單元(GRU)
- 9.2. 長短期記憶網路(LSTM)
- 9.3. 深度循環神經網路
- 9.4. 雙向循環神經網路
- 9.5. 機器翻譯與資料集
- 9.6. 編碼器-解碼器架構
- 9.7. 序列到序列學習(seq2seq)
- 9.8. 束搜索

### 10_注意力機制
- 10.1. 注意力提示
- 10.2. 注意力匯聚:Nadaraya-Watson核回歸
- 10.3. 注意力評分函數
- 10.4. Bahdanau 注意力
- 10.5. 多頭注意力
- 10.6. 自注意力和位置編碼
- 10.7. Transformer

### 11_優化算法
- 11.1. 優化與深度學習
- 11.2. 凸性
- 11.3. 梯度下降
- 11.4. 隨機梯度下降
- 11.5. 小批量隨機梯度下降
- 11.6. 動量法
- 11.7. AdaGrad算法
- 11.8. RMSProp算法
- 11.9. Adadelta
- 11.10. Adam算法
- 11.11. 學習率調度器

### 12_計算性能
- 12.1. 編譯器與解釋器
- 12.2. 異步計算
- 12.3. 自動並行
- 12.4. 硬體
- 12.5. 多GPU訓練
- 12.6. 多GPU的簡潔實現
- 12.7. 參數服務器

### 13_計算機視覺
- 13.1. 圖像增廣
- 13.2. 微調
- 13.3. 目標檢測與邊界框
- 13.4. 錨框
- 13.5. 多尺度目標檢測
- 13.6. 目標檢測資料集
- 13.7. 單發多框檢測(SSD)
- 13.8. 區域卷積神經網路(R-CNN)系列
- 13.9. 語意分割與資料集
- 13.10. 轉置卷積
- 13.11. 全卷積網路
- 13.12. 風格遷移
- 13.13. 實戰Kaggle比賽:圖像分類(CIFAR-10)
- 13.14. 實戰Kaggle比賽:狗品種識別(CIFAR-10)

### 14_自然語言處理:預訓練
- 14.1. 詞嵌入(word2vec)
- 14.2. 近似訓練
- 14.3. 用於預訓練詞嵌入的資料集
- 14.4. 預訓練word2vec
- 14.5. 全局向量的詞嵌入(GloVe)
- 14.6. 子詞嵌入
- 14.7. 詞的相似性和類比任務
- 14.8. 來自Transformers的雙向編碼器表示(BERT)
- 14.9. 用於預訓練BERT的資料集
- 14.10. 預訓練BERT


### 15_自然語言處理:應用
- 15.1. 情感分析及資料集
- 15.2. 情感分析:使用循環神經網路
- 15.3. 情感分析:使用卷積神經網路
- 15.4. 自然語言推斷與資料集
- 15.5. 自然語言推斷: 使用注意力
- 15.6. 針對序列級和詞源級應用微調BERT
- 15.7. 自然語言推斷: 微調BERT