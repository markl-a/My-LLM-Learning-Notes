{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4873f83",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 自動微分\n",
    ":label:`sec_autograd`\n",
    " \n",
    "正如 :numref:`sec_calculus`中所說，求導是幾乎所有深度學習優化算法的關鍵步驟。\n",
    "雖然求導的計算很簡單，只需要一些基本的微積分。\n",
    "但對於複雜的模型，手工進行更新是一件很痛苦的事情（而且經常容易出錯）。\n",
    " \n",
    "深度學習框架通過自動計算導數，即*自動微分*（automatic differentiation）來加快求導。\n",
    "實際中，根據設計好的模型，系統會構建一個*計算圖*（computational graph），\n",
    "來追蹤計算是哪些數據通過哪些操作組合起來產生輸出。\n",
    "自動微分使系統能夠隨後反向傳播梯度。\n",
    "這裡，*反向傳播*（backpropagate）意味著追蹤整個計算圖，填充關於每個參數的偏導數。\n",
    "\n",
    "## 一個簡單的例子\n",
    " \n",
    "作為一個演示例子，(**假設我們想對函數$y=2\\mathbf{x}^{\\top}\\mathbf{x}$關於列向量$\\mathbf{x}$求導**)。\n",
    "首先，我們創建變量`x`並為其分配一個初始值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98cd8a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:31.627945Z",
     "iopub.status.busy": "2023-08-18T07:07:31.627424Z",
     "iopub.status.idle": "2023-08-18T07:07:32.686372Z",
     "shell.execute_reply": "2023-08-18T07:07:32.685559Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec430520",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "[**在我們計算$y$關於$\\mathbf{x}$的梯度之前，需要一個地方來儲存梯度。**]\n",
    "重要的是，我們不會在每次對一個參數求導時都分配新的記憶體。\n",
    "因為我們經常會成千上萬次地更新相同的參數，每次都分配新的記憶體可能很快就會將記憶體耗盡。\n",
    "注意，一個純量函數關於向量$\\mathbf{x}$的梯度是向量，並且與$\\mathbf{x}$具有相同的形狀。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27a5df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.690633Z",
     "iopub.status.busy": "2023-08-18T07:07:32.689882Z",
     "iopub.status.idle": "2023-08-18T07:07:32.694159Z",
     "shell.execute_reply": "2023-08-18T07:07:32.693367Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "x.requires_grad_(True)  # 等價於x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # 默認值是None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd993524",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "(**現在計算$y$。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c3f80b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.698006Z",
     "iopub.status.busy": "2023-08-18T07:07:32.697167Z",
     "iopub.status.idle": "2023-08-18T07:07:32.705385Z",
     "shell.execute_reply": "2023-08-18T07:07:32.704593Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35523dbc",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "`x`是一個長度為4的向量，計算`x`和`x`的點積，得到了我們賦值給`y`的標量輸出。\n",
    "接下來，[**通過調用反向傳播函數來自動計算`y`關於`x`每個分量的梯度**]，並打印這些梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c3a419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.708698Z",
     "iopub.status.busy": "2023-08-18T07:07:32.708196Z",
     "iopub.status.idle": "2023-08-18T07:07:32.713924Z",
     "shell.execute_reply": "2023-08-18T07:07:32.713091Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6a271",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "函數$y=2\\mathbf{x}^{\\top}\\mathbf{x}$關於$\\mathbf{x}$的梯度應為$4\\mathbf{x}$。\n",
    "讓我們快速驗證這個梯度是否計算正確。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8493d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.718858Z",
     "iopub.status.busy": "2023-08-18T07:07:32.718156Z",
     "iopub.status.idle": "2023-08-18T07:07:32.724091Z",
     "shell.execute_reply": "2023-08-18T07:07:32.723104Z"
    },
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733c623",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "[**現在計算`x`的另一個函數。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fcd392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.729368Z",
     "iopub.status.busy": "2023-08-18T07:07:32.728433Z",
     "iopub.status.idle": "2023-08-18T07:07:32.736493Z",
     "shell.execute_reply": "2023-08-18T07:07:32.735715Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在預設情況下，PyTorch會累積梯度，我們需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4f459",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 非純量變量的反向傳播\n",
    " \n",
    "當`y`不是純量時，向量`y`關於向量`x`的導數的最自然解釋是一個矩陣。\n",
    "對於高階和高維的`y`和`x`，求導的結果可以是一個高階張量。\n",
    " \n",
    "然而，雖然這些更奇特的物件確實出現在高級機器學習中（包括[**深度學習中**]），\n",
    "但當調用向量的反向計算時，我們通常會試圖計算一批訓練樣本中每個組成部分的損失函數的導數。\n",
    "這裡(**，我們的目的不是計算微分矩陣，而是單獨計算批量中每個樣本的偏導數之和。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e62a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.740109Z",
     "iopub.status.busy": "2023-08-18T07:07:32.739419Z",
     "iopub.status.idle": "2023-08-18T07:07:32.745803Z",
     "shell.execute_reply": "2023-08-18T07:07:32.744893Z"
    },
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 對非純量調用backward需要傳入一個gradient參數，該參數指定微分函數關於self的梯度。\n",
    "# 本例只想求偏導數的和，所以傳遞一個1的梯度是合適的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等價於y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f510c4",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "## 分離計算\n",
    " \n",
    "有時，我們希望[**將某些計算移動到記錄的計算圖之外**]。\n",
    "例如，假設`y`是作為`x`的函數計算的，而`z`則是作為`y`和`x`的函數計算的。\n",
    "想像一下，我們想計算`z`關於`x`的梯度，但由於某種原因，希望將`y`視為一個常數，\n",
    "並且只考慮到`x`在`y`被計算後發揮的作用。\n",
    " \n",
    "這裡可以分離`y`來返回一個新變量`u`，該變量與`y`具有相同的值，\n",
    "但丟棄計算圖中如何計算`y`的任何信息。\n",
    "換句話說，梯度不會向後流經`u`到`x`。\n",
    "因此，下面的反向傳播函數計算`z=u*x`關於`x`的偏導數，同時將`u`作為常數處理，\n",
    "而不是`z=x*x*x`關於`x`的偏導數。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dab493d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.749398Z",
     "iopub.status.busy": "2023-08-18T07:07:32.748759Z",
     "iopub.status.idle": "2023-08-18T07:07:32.755280Z",
     "shell.execute_reply": "2023-08-18T07:07:32.754543Z"
    },
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe6f9c",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "由於記錄了`y`的計算結果，我們可以隨後在`y`上調用反向傳播，\n",
    "得到`y=x*x`關於的`x`的導數，即`2*x`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "271a9b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.759344Z",
     "iopub.status.busy": "2023-08-18T07:07:32.758633Z",
     "iopub.status.idle": "2023-08-18T07:07:32.764663Z",
     "shell.execute_reply": "2023-08-18T07:07:32.763922Z"
    },
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79d12f",
   "metadata": {
    "origin_pos": 45
   },
   "source": [
    "## Python控制流的梯度計算\n",
    " \n",
    "使用自動微分的一個好處是：\n",
    "[**即使構建函數的計算圖需要通過Python控制流（例如，條件、循環或任意函數調用），我們仍然可以計算得到的變量的梯度**]。\n",
    "在下面的代碼中，`while`循環的迭代次數和`if`語句的結果都取決於輸入`a`的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6323b2ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.769249Z",
     "iopub.status.busy": "2023-08-18T07:07:32.768616Z",
     "iopub.status.idle": "2023-08-18T07:07:32.773175Z",
     "shell.execute_reply": "2023-08-18T07:07:32.772293Z"
    },
    "origin_pos": 47,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aaf333",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "讓我們計算梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7719d6b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.777740Z",
     "iopub.status.busy": "2023-08-18T07:07:32.777207Z",
     "iopub.status.idle": "2023-08-18T07:07:32.782254Z",
     "shell.execute_reply": "2023-08-18T07:07:32.781458Z"
    },
    "origin_pos": 52,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a1ac2",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "我們現在可以分析上面定義的`f`函數。\n",
    "請注意，它在其輸入`a`中是分段線性的。\n",
    "換言之，對於任何`a`，存在某個常量標量`k`，使得`f(a)=k*a`，其中`k`的值取決於輸入`a`，因此可以用`d/a`驗證梯度是否正確。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2595bdc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:32.785728Z",
     "iopub.status.busy": "2023-08-18T07:07:32.785179Z",
     "iopub.status.idle": "2023-08-18T07:07:32.790672Z",
     "shell.execute_reply": "2023-08-18T07:07:32.789892Z"
    },
    "origin_pos": 57,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb5517",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "## 小結\n",
    " \n",
    "* 深度學習框架可以自動計算導數：我們首先將梯度附加到想要對其計算偏導數的變量上，然後記錄目標值的計算，執行它的反向傳播函數，並訪問得到的梯度。\n",
    "\n",
    "## 練習\n",
    " \n",
    "1. 為什麼計算二階導數比一階導數的開銷要更大？\n",
    "1. 在運行反向傳播函數之後，立即再次運行它，看看會發生什麼。\n",
    "1. 在控制流的例子中，我們計算`d`關於`a`的導數，如果將變量`a`更改為隨機向量或矩陣，會發生什麼？\n",
    "1. 重新設計一個求控制流梯度的例子，運行並分析結果。\n",
    "1. 使$f(x)=\\sin(x)$，繪製$f(x)$和$\\frac{df(x)}{dx}$的圖像，其中後者不使用$f'(x)=\\cos(x)$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f74f8",
   "metadata": {
    "origin_pos": 62,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1759)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1f865",
   "metadata": {},
   "source": [
    "回答(不一定正確):\n",
    "\n",
    "1. 為什麼計算二階導數比一階導數的開銷要更大？\n",
    "---\n",
    "\n",
    "計算二階導數比一階導數的開銷更大，主要是由以下原因導致的：\n",
    "\n",
    "1. **額外的計算依賴性**：\n",
    "   - 一階導數僅需要計算函數對輸入的偏導數。\n",
    "   - 二階導數則需要計算一階導數對輸入的導數，因此需要追蹤和存儲更多的計算圖。\n",
    "   \n",
    "2. **計算圖的大小增加**：\n",
    "   - 自動微分中，計算圖會表示所有變數之間的依賴性。計算二階導數時，圖的深度和複雜性增加，需要更多內存和計算資源。\n",
    "\n",
    "3. **額外的反向傳播步驟**：\n",
    "   - 計算一階導數需要執行一次反向傳播。而計算二階導數需要在一階導數的基礎上再次進行反向傳播。\n",
    "\n",
    "4. **數值穩定性**：\n",
    "   - 高階導數的計算更容易受到數值誤差的影響，可能需要更多精確的浮點運算來避免不穩定的結果。\n",
    "\n",
    "計算二階導數涉及更多的計算依賴性、更大的計算圖和額外的反向傳播步驟，因此比計算一階導數的開銷更大。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf539e0c",
   "metadata": {},
   "source": [
    "---\n",
    "2. 在運行反向傳播函數之後，立即再次運行它，看看會發生什麼。\n",
    "---\n",
    "\n",
    "在 PyTorch 中執行反向傳播（`backward()`）後，計算圖默認會被釋放，因為 PyTorch 的計算圖僅用於一次性反向傳播。這是為了節省內存空間。\n",
    "\n",
    "因此，嘗試在執行完一次 `backward()` 後再次執行，會引發錯誤：\n",
    "```\n",
    "RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed.\n",
    "```\n",
    "\n",
    "要避免這種情況，您可以使用 `retain_graph=True` 在第一次反向傳播時保留計算圖，以便之後可以再次使用。\n",
    "\n",
    "---\n",
    "\n",
    "### **程式碼範例：**\n",
    "這段程式碼模擬問題，並解決它。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 建立需要梯度的張量\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = x.pow(2).sum()  # y = x1^2 + x2^2\n",
    "\n",
    "# 第一次反向傳播\n",
    "y.backward(retain_graph=True)  # 保留計算圖\n",
    "print(\"第一次反向傳播後的梯度：\", x.grad)\n",
    "\n",
    "# 嘗試再次反向傳播\n",
    "try:\n",
    "    y.backward()  # 再次反向傳播（若未保留計算圖，會報錯）\n",
    "    print(\"第二次反向傳播後的梯度：\", x.grad)\n",
    "except RuntimeError as e:\n",
    "    print(\"再次反向傳播時的錯誤：\", e)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **分析結果：**\n",
    "1. **第一次反向傳播**：\n",
    "   - 會計算梯度並存儲在 `x.grad` 中。\n",
    "2. **第二次反向傳播**：\n",
    "   - 如果未使用 `retain_graph=True`，將會引發錯誤。\n",
    "   - 若使用 `retain_graph=True`，可以成功執行，但需注意內存開銷。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082312c0",
   "metadata": {},
   "source": [
    "---\n",
    "3. 在控制流的例子中，我們計算`d`關於`a`的導數，如果將變量`a`更改為隨機向量或矩陣，會發生什麼？\n",
    "---\n",
    "1. **梯度的重新計算**：\n",
    "   - 如果將變量 `a` 更改為隨機向量或矩陣，計算 `d` 關於 `a` 的導數時，自動微分工具（如 PyTorch）將會重新構建計算圖並計算梯度。梯度的結果會根據 `a` 的新值而變化。\n",
    "\n",
    "2. **動態計算圖特性**：\n",
    "   - PyTorch 的動態計算圖允許對輸入值的靈活更改。當 `a` 被更改為隨機向量或矩陣後，新的計算會基於新值生成對應的計算圖，並計算梯度。\n",
    "\n",
    "3. **結果的數學意義**：\n",
    "   - 對於隨機輸入，梯度的具體數值會因隨機輸入的值不同而不同，但導數的計算方式（基於函數的偏導）仍然一致。\n",
    "\n",
    "---\n",
    "\n",
    "### **程式碼範例：**\n",
    "\n",
    "以下是一段 PyTorch 程式碼來測試這個情境：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 定義一個控制流函數\n",
    "def control_flow_example(a):\n",
    "    b = a * 2 if a.sum() > 1 else a / 2\n",
    "    return b.sum()\n",
    "\n",
    "# 初始化變量 a，並啟用梯度\n",
    "a = torch.tensor([0.5, 1.0, -0.5], requires_grad=True)\n",
    "\n",
    "# 計算控制流函數的輸出\n",
    "d = control_flow_example(a)\n",
    "d.backward()  # 計算 d 關於 a 的導數\n",
    "print(\"原始梯度：\", a.grad)\n",
    "\n",
    "# 更改變量 a 為隨機向量，重新計算\n",
    "a = torch.randn(3, requires_grad=True)\n",
    "d = control_flow_example(a)\n",
    "d.backward()\n",
    "print(\"隨機向量的新梯度：\", a.grad)\n",
    "\n",
    "# 更改變量 a 為隨機矩陣，重新計算\n",
    "a = torch.randn(3, 3, requires_grad=True)\n",
    "d = control_flow_example(a)\n",
    "d.backward()\n",
    "print(\"隨機矩陣的新梯度：\", a.grad)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **分析結果：**\n",
    "1. **梯度值的變化**：\n",
    "   - 由於 `a` 的值改變，`control_flow_example` 中的條件分支可能會選擇不同的計算路徑，因此導致梯度值的變化。\n",
    "2. **計算圖的重新構建**：\n",
    "   - 每次變更 `a` 的值後，PyTorch 會根據新值重新構建計算圖，確保正確計算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922eed8",
   "metadata": {},
   "source": [
    "---\n",
    "4. 重新設計一個求控制流梯度的例子，運行並分析結果。\n",
    "---\n",
    "\n",
    "### 回答：\n",
    "\n",
    "1. **控制流梯度的特性**：\n",
    "   - 控制流梯度的計算會根據輸入數值的不同而觸發不同的計算路徑。梯度值會反映選擇的計算路徑，因此它可能是分段的。\n",
    "\n",
    "2. **重新設計的例子**：\n",
    "   - 設計一個包含條件分支的函數，條件基於輸入值。\n",
    "   - 計算梯度，並展示梯度如何根據控制流的不同選擇而改變。\n",
    "\n",
    "3. **分析結果**：\n",
    "   - 不同的控制流路徑會影響梯度值。\n",
    "   - 梯度反映了當前輸入值所觸發的計算路徑的偏導。\n",
    "\n",
    "---\n",
    "\n",
    "### **程式碼範例**：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 定義控制流函數\n",
    "def custom_control_flow(x):\n",
    "    # 根據條件選擇不同計算路徑\n",
    "    if x.sum() > 0:\n",
    "        y = x ** 2  # 條件 1: 平方\n",
    "    else:\n",
    "        y = x.abs()  # 條件 2: 絕對值\n",
    "    return y.sum()\n",
    "\n",
    "# 測試 1: x 為正向量\n",
    "x = torch.tensor([1.0, 2.0, -3.0], requires_grad=True)\n",
    "output = custom_control_flow(x)\n",
    "output.backward()\n",
    "print(\"x 為正向量時的梯度：\", x.grad)\n",
    "\n",
    "# 測試 2: x 為負向量\n",
    "x = torch.tensor([-1.0, -2.0, -3.0], requires_grad=True)\n",
    "output = custom_control_flow(x)\n",
    "output.backward()\n",
    "print(\"x 為負向量時的梯度：\", x.grad)\n",
    "\n",
    "# 測試 3: x 為隨機值\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "output = custom_control_flow(x)\n",
    "output.backward()\n",
    "print(\"x 為隨機值時的梯度：\", x.grad)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **分析結果**：\n",
    "\n",
    "1. **條件選擇影響梯度**：\n",
    "   - 當 `x.sum() > 0` 時，梯度反映的是平方運算的導數（`2*x`）。\n",
    "   - 當 `x.sum() <= 0` 時，梯度反映的是絕對值的導數（`1` 或 `-1`，取決於符號）。\n",
    "\n",
    "2. **控制流的動態性**：\n",
    "   - 計算圖動態調整，以匹配當前輸入值觸發的分支路徑。\n",
    "\n",
    "3. **重新設計的目的**：\n",
    "   - 這例子展示了控制流的條件如何影響梯度計算的路徑和結果，幫助理解動態計算圖的優勢。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed0235",
   "metadata": {},
   "source": [
    "---\n",
    "5. 使$f(x)=\\sin(x)$，繪製$f(x)$和$\\frac{df(x)}{dx}$的圖像，其中後者不使用$f'(x)=\\cos(x)$。\n",
    "---\n",
    "\n",
    "### 回答：\n",
    "\n",
    "1. **計算 \\( \\frac{df(x)}{dx} \\) 的方法**：\n",
    "   - 使用自動微分框架（如 PyTorch）計算梯度，而不是直接使用 \\( f'(x) = \\cos(x) \\) 的公式。\n",
    "   - 梯度通過 `torch.autograd` 自動計算。\n",
    "\n",
    "2. **繪製圖像**：\n",
    "   - 使用 Matplotlib 繪製 \\( f(x) = \\sin(x) \\) 和 \\( \\frac{df(x)}{dx} \\)。\n",
    "   - 梯度的數值會來自於自動微分計算。\n",
    "\n",
    "3. **結果分析**：\n",
    "   - \\( \\sin(x) \\) 的導數應該與 \\( \\cos(x) \\) 的圖像一致，但計算過程依賴於自動微分而非顯式公式。\n",
    "\n",
    "---\n",
    "\n",
    "### **程式碼範例**：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 定義 x 範圍\n",
    "x_vals = np.linspace(-2 * np.pi, 2 * np.pi, 100)\n",
    "x = torch.tensor(x_vals, requires_grad=True)\n",
    "\n",
    "# 定義 f(x) = sin(x)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 計算梯度\n",
    "y.backward(torch.ones_like(x))  # 對 y 求梯度\n",
    "grad = x.grad  # 取得梯度值\n",
    "\n",
    "# 繪製圖像\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 繪製 f(x)\n",
    "plt.plot(x_vals, np.sin(x_vals), label=\"f(x) = sin(x)\")\n",
    "\n",
    "# 繪製 df(x)/dx\n",
    "plt.plot(x_vals, grad.detach().numpy(), label=\"df(x)/dx (calculated via autograd)\")\n",
    "\n",
    "# 標註\n",
    "plt.title(\"f(x) = sin(x) and its derivative df(x)/dx\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axhline(0, color='gray', linewidth=0.5, linestyle='--')\n",
    "plt.axvline(0, color='gray', linewidth=0.5, linestyle='--')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **結果分析**：\n",
    "\n",
    "1. **圖像的對比**：\n",
    "   - \\( f(x) = \\sin(x) \\) 為正弦曲線。\n",
    "   - \\( \\frac{df(x)}{dx} \\) 的自動微分結果應與 \\( \\cos(x) \\) 完全一致。\n",
    "\n",
    "2. **不使用顯式公式**：\n",
    "   - 梯度的計算純粹依賴於自動微分，驗證了框架的正確性和動態計算圖的強大功能。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc6029",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
