{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af409a8",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 線性代數\n",
    ":label:`sec_linear-algebra`\n",
    "\n",
    "在介紹完如何儲存和操作數據後，接下來將簡要地回顧一下部分基本線性代數內容。\n",
    "這些內容有助於讀者了解和實現本書中介紹的大多數模型。\n",
    "本節將介紹線性代數中的基本數學物件、算術和運算，並用數學符號和相應的程式碼實現來表示它們。\n",
    "\n",
    "## 純量\n",
    "\n",
    "如果你曾經在餐廳支付餐費，那麼應該已經知道一些基本的線性代數，比如在數字間相加或相乘。\n",
    "例如，北京的溫度為$52^{\\circ}F$（華氏度，除攝氏度外的另一種溫度計量單位）。\n",
    "嚴格來說，僅包含一個數值被稱為*純量*（scalar）。\n",
    "如果要將此華氏度值轉換為更常用的攝氏度，\n",
    "則可以計算表達式$c=\\frac{5}{9}(f-32)$，並將$f$賦為$52$。\n",
    "在此等式中，每一項（$5$、$9$和$32$）都是純量值。\n",
    "符號$c$和$f$稱為*變數*（variable），它們表示未知的純量值。\n",
    "\n",
    "本書採用了數學表示法，其中純量變數由普通小寫字母表示（例如，$x$、$y$和$z$）。\n",
    "本書用$\\mathbb{R}$表示所有（連續）*實數*純量的空間，之後將嚴格定義*空間*（space）是什麼，\n",
    "但現在只要記住表達式$x\\in\\mathbb{R}$是表示$x$是一個實值純量的正式形式。\n",
    "符號$\\in$稱為「屬於」，它表示「是集合中的成員」。\n",
    "例如$x, y \\in \\{0,1\\}$可以用來表明$x$和$y$是值只能為$0$或$1$的數字。\n",
    "\n",
    "(**純量由只有一個元素的張量表示**)。\n",
    "下面的程式碼將實例化兩個純量，並執行一些熟悉的算術運算，即加法、乘法、除法和指數。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44889577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:42.354657Z",
     "iopub.status.busy": "2023-08-18T07:01:42.353844Z",
     "iopub.status.idle": "2023-08-18T07:01:43.769394Z",
     "shell.execute_reply": "2023-08-18T07:01:43.768177Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bf250",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## 向量\n",
    " \n",
    "[**向量可以被視為純量值組成的列表**]。\n",
    "這些純量值被稱為向量的*元素*（element）或*分量*（component）。\n",
    "當向量表示數據集中的樣本時，它們的值具有一定的現實意義。\n",
    "例如，如果我們正在訓練一個模型來預測貸款違約風險，可能會將每個申請人與一個向量相關聯，\n",
    "其分量與其收入、工作年限、過往違約次數和其他因素相對應。\n",
    "如果我們正在研究醫院患者可能面臨的心臟病發作風險，可能會用一個向量來表示每個患者，\n",
    "其分量為最近的生命體徵、膽固醇水平、每天運動時間等。\n",
    "在數學表示法中，向量通常記為粗體、小寫的符號\n",
    "（例如，$\\mathbf{x}$、$\\mathbf{y}$和$\\mathbf{z})$）。\n",
    "\n",
    "人們通過一維張量表示向量。一般來說，張量可以具有任意長度，取決於機器的記憶體限制。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5163ab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.774490Z",
     "iopub.status.busy": "2023-08-18T07:01:43.773987Z",
     "iopub.status.idle": "2023-08-18T07:01:43.781757Z",
     "shell.execute_reply": "2023-08-18T07:01:43.780603Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8cd94",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "我們可以使用下標來引用向量的任一元素，例如可以通過$x_i$來引用第$i$個元素。\n",
    "注意，元素$x_i$是一個純量，所以我們在引用它時不會加粗。\n",
    "大量文獻認為列向量是向量的預設方向，在本書中也是如此。\n",
    "在數學中，向量$\\mathbf{x}$可以寫為：\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
    ":eqlabel:`eq_vec_def`\n",
    "\n",
    "其中$x_1,\\ldots,x_n$是向量的元素。在程式碼中，我們(**通過張量的索引來存取任一元素**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34dd7630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.786346Z",
     "iopub.status.busy": "2023-08-18T07:01:43.785939Z",
     "iopub.status.idle": "2023-08-18T07:01:43.793065Z",
     "shell.execute_reply": "2023-08-18T07:01:43.791986Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e98e89",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "### 長度、維度和形狀\n",
    "\n",
    "向量只是一個數字陣列，就像每個陣列都有一個長度一樣，每個向量也是如此。\n",
    "在數學表示法中，如果我們想說一個向量$\\mathbf{x}$由$n$個實值純量組成，\n",
    "可以將其表示為$\\mathbf{x}\\in\\mathbb{R}^n$。\n",
    "向量的長度通常稱為向量的*維度*（dimension）。\n",
    " \n",
    "與普通的Python陣列一樣，我們可以透過呼叫Python的內建`len()`函數來[**存取張量的長度**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d469059b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.798087Z",
     "iopub.status.busy": "2023-08-18T07:01:43.797197Z",
     "iopub.status.idle": "2023-08-18T07:01:43.804049Z",
     "shell.execute_reply": "2023-08-18T07:01:43.802867Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0ccc4",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "當用張量表示一個向量（只有一個軸）時，我們也可以通過`.shape`屬性訪問向量的長度。\n",
    "形狀（shape）是一個元素組，列出了張量沿每個軸的長度（維數）。\n",
    "對於(**只有一個軸的張量，形狀只有一個元素。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf9bf15e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.809543Z",
     "iopub.status.busy": "2023-08-18T07:01:43.808709Z",
     "iopub.status.idle": "2023-08-18T07:01:43.815762Z",
     "shell.execute_reply": "2023-08-18T07:01:43.814675Z"
    },
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237aeca",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "請注意，*維度*（dimension）這個詞在不同上下文時往往會有不同的含義，這經常會使人感到困惑。\n",
    "為了清楚起見，我們在此明確一下：\n",
    "*向量*或*軸*的維度被用來表示*向量*或*軸*的長度，即向量或軸的元素數量。\n",
    "然而，張量的維度用來表示張量具有的軸數。\n",
    "在這個意義上，張量的某個軸的維數就是這個軸的長度。\n",
    " \n",
    "## 矩陣\n",
    "\n",
    "正如向量將標量從零階推廣到一階，矩陣將向量從一階推廣到二階。\n",
    "矩陣，我們通常用粗體、大寫字母來表示\n",
    "（例如，$\\mathbf{X}$、$\\mathbf{Y}$和$\\mathbf{Z}$），\n",
    "在程式碼中表示為具有兩個軸的張量。\n",
    " \n",
    "數學表示法使用$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "來表示矩陣$\\mathbf{A}$，其由$m$行和$n$列的實值標量組成。\n",
    "我們可以將任意矩陣$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$視為一個表格，\n",
    "其中每個元素$a_{ij}$屬於第$i$行第$j$列：\n",
    " \n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n",
    ":eqlabel:`eq_matrix_def`\n",
    " \n",
    "對於任意$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$，\n",
    "$\\mathbf{A}$的形狀是（$m$,$n$）或$m \\times n$。\n",
    "當矩陣具有相同數量的行和列時，其形狀將變為正方形；\n",
    "因此，它被稱為*方陣*（square matrix）。\n",
    " \n",
    "當呼叫函數來實例化張量時，\n",
    "我們可以[**通過指定兩個分量$m$和$n$來創建一個形狀為$m \\times n$的矩陣**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1eac085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.820985Z",
     "iopub.status.busy": "2023-08-18T07:01:43.820088Z",
     "iopub.status.idle": "2023-08-18T07:01:43.828057Z",
     "shell.execute_reply": "2023-08-18T07:01:43.826957Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5181b",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "我們可以通過行索引（$i$）和列索引（$j$）來訪問矩陣中的標量元素$a_{ij}$，\n",
    "例如$[\\mathbf{A}]_{ij}$。\n",
    "如果沒有給出矩陣$\\mathbf{A}$的標量元素，如在 :eqref:`eq_matrix_def`那樣，\n",
    "我們可以簡單地使用矩陣$\\mathbf{A}$的小寫字母索引下標$a_{ij}$\n",
    "來引用$[\\mathbf{A}]_{ij}$。\n",
    "為了表示起來簡單，只有在必要時才會將逗號插入到單獨的索引中，\n",
    "例如$a_{2,3j}$和$[\\mathbf{A}]_{2i-1,3}$。\n",
    " \n",
    "當我們交換矩陣的行和列時，結果稱為矩陣的*轉置*（transpose）。\n",
    "通常用$\\mathbf{a}^\\top$來表示矩陣的轉置，如果$\\mathbf{B}=\\mathbf{A}^\\top$，\n",
    "則對於任意$i$和$j$，都有$b_{ij}=a_{ji}$。\n",
    "因此，在 :eqref:`eq_matrix_def`中的轉置是一個形狀為$n \\times m$的矩陣：\n",
    " \n",
    "$$\n",
    "\\mathbf{A}^\\top =\n",
    "\\begin{bmatrix}\n",
    "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
    "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    " \n",
    "現在在程式碼中訪問(**矩陣的轉置**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "289523ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.833285Z",
     "iopub.status.busy": "2023-08-18T07:01:43.832377Z",
     "iopub.status.idle": "2023-08-18T07:01:43.839757Z",
     "shell.execute_reply": "2023-08-18T07:01:43.838656Z"
    },
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce004f",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "作為方陣的一種特殊類型，[***對稱矩陣*（symmetric matrix）$\\mathbf{A}$等於其轉置：$\\mathbf{A} = \\mathbf{A}^\\top$**]。\n",
    "這裡定義一個對稱矩陣$\\mathbf{B}$：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0eb414b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.845394Z",
     "iopub.status.busy": "2023-08-18T07:01:43.844475Z",
     "iopub.status.idle": "2023-08-18T07:01:43.852725Z",
     "shell.execute_reply": "2023-08-18T07:01:43.851678Z"
    },
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d810164",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "現在我們將`B`與它的轉置進行比較。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44cc700c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.857930Z",
     "iopub.status.busy": "2023-08-18T07:01:43.856978Z",
     "iopub.status.idle": "2023-08-18T07:01:43.864388Z",
     "shell.execute_reply": "2023-08-18T07:01:43.863329Z"
    },
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5d327",
   "metadata": {
    "origin_pos": 45
   },
   "source": [
    "矩陣是有用的數據結構：它們允許我們組織具有不同模式的數據。\n",
    "例如，我們矩陣中的行可能對應於不同的房屋（數據樣本），而列可能對應於不同的屬性。\n",
    "曾經使用過電子表格軟體或已閱讀過 :numref:`sec_pandas`的人，應該對此很熟悉。\n",
    "因此，儘管單個向量的預設方向是列向量，但在表示表格數據集的矩陣中，\n",
    "將每個數據樣本作為矩陣中的行向量更為常見。\n",
    "後面的章節將講到這點，這種約定將支援常見的深度學習實踐。\n",
    "例如，沿著張量的最外軸，我們可以訪問或遍歷小批量的數據樣本。\n",
    " \n",
    " \n",
    "## 張量\n",
    "\n",
    "[**就像向量是標量的推廣，矩陣是向量的推廣一樣，我們可以構建具有更多軸的數據結構**]。\n",
    "張量（本小節中的「張量」指代數對象）是描述具有任意數量軸的$n$維陣列的通用方法。\n",
    "例如，向量是一階張量，矩陣是二階張量。\n",
    "張量用特殊字體的大寫字母表示（例如，$\\mathsf{X}$、$\\mathsf{Y}$和$\\mathsf{Z}$），\n",
    "它們的索引機制（例如$x_{ijk}$和$[\\mathsf{X}]_{1,2i-1,3}$）與矩陣類似。\n",
    " \n",
    "當我們開始處理圖像時，張量將變得更加重要，圖像以$n$維陣列形式出現，\n",
    "其中3個軸對應於高度、寬度，以及一個*通道*（channel）軸，\n",
    "用於表示顏色通道（紅色、綠色和藍色）。\n",
    "現在先將高階張量暫放一邊，而是專注學習其基礎知識。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f7227a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.869592Z",
     "iopub.status.busy": "2023-08-18T07:01:43.868624Z",
     "iopub.status.idle": "2023-08-18T07:01:43.876563Z",
     "shell.execute_reply": "2023-08-18T07:01:43.875497Z"
    },
    "origin_pos": 47,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003e028",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "## 張量算法的基本性質\n",
    " \n",
    "標量、向量、矩陣和任意數量軸的張量（本小節中的「張量」指代數對象）有一些實用的屬性。\n",
    "例如，從按元素操作的定義中可以注意到，任何按元素的一元運算都不會改變其操作數的形狀。\n",
    "同樣，[**給定具有相同形狀的任意兩個張量，任何按元素二元運算的結果都將是相同形狀的張量**]。\n",
    "例如，將兩個相同形狀的矩陣相加，會在這兩個矩陣上執行元素加法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6c89bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.881686Z",
     "iopub.status.busy": "2023-08-18T07:01:43.880912Z",
     "iopub.status.idle": "2023-08-18T07:01:43.891206Z",
     "shell.execute_reply": "2023-08-18T07:01:43.890082Z"
    },
    "origin_pos": 52,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # 通过分配新内存，将A的一个副本分配给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f634ff0",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "具體而言，[**兩個矩陣的按元素乘法稱為*Hadamard積*（Hadamard product）（數學符號$\\odot$）**]。\n",
    "對於矩陣$\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$，\n",
    "其中第$i$行和第$j$列的元素是$b_{ij}$。\n",
    "矩陣$\\mathbf{A}$（在 :eqref:`eq_matrix_def`中定義）和$\\mathbf{B}$的Hadamard積為：\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1efe4855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.896102Z",
     "iopub.status.busy": "2023-08-18T07:01:43.895401Z",
     "iopub.status.idle": "2023-08-18T07:01:43.903331Z",
     "shell.execute_reply": "2023-08-18T07:01:43.902251Z"
    },
    "origin_pos": 57,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1666f",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "將張量乘以或加上一個標量不會改變張量的形狀，其中張量的每個元素都將與標量相加或相乘。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "587335a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.908593Z",
     "iopub.status.busy": "2023-08-18T07:01:43.907694Z",
     "iopub.status.idle": "2023-08-18T07:01:43.916299Z",
     "shell.execute_reply": "2023-08-18T07:01:43.915117Z"
    },
    "origin_pos": 62,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fee7fa",
   "metadata": {
    "origin_pos": 65
   },
   "source": [
    "## 降維\n",
    " \n",
    ":label:`subseq_lin-alg-reduction`\n",
    " \n",
    "我們可以對任意張量進行的一個有用的操作是[**計算其元素的和**]。\n",
    "數學表示法使用$\\sum$符號表示求和。\n",
    "為了表示長度為$d$的向量中元素的總和，可以記為$\\sum_{i=1}^dx_i$。\n",
    "在程式碼中可以呼叫計算求和的函數：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32507943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.921298Z",
     "iopub.status.busy": "2023-08-18T07:01:43.920499Z",
     "iopub.status.idle": "2023-08-18T07:01:43.929213Z",
     "shell.execute_reply": "2023-08-18T07:01:43.928096Z"
    },
    "origin_pos": 67,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f651b",
   "metadata": {
    "origin_pos": 70
   },
   "source": [
    "我們可以(**表示任意形狀張量的元素和**)。\n",
    "例如，矩陣$\\mathbf{A}$中元素的和可以記為$\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e0cd60f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.934058Z",
     "iopub.status.busy": "2023-08-18T07:01:43.933342Z",
     "iopub.status.idle": "2023-08-18T07:01:43.940936Z",
     "shell.execute_reply": "2023-08-18T07:01:43.939832Z"
    },
    "origin_pos": 72,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62dab82",
   "metadata": {
    "origin_pos": 75
   },
   "source": [
    "預設情況下，呼叫求和函數會沿所有的軸降低張量的維度，使它變為一個純量。\n",
    "我們還可以[**指定張量沿哪一個軸來透過求和降低維度**]。\n",
    "以矩陣為例，為了透過求和所有列的元素來降維（軸0），可以在呼叫函數時指定`axis=0`。\n",
    "由於輸入矩陣沿0軸降維以生成輸出向量，因此輸入軸0的維數在輸出形狀中消失。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9420cc92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.946290Z",
     "iopub.status.busy": "2023-08-18T07:01:43.945345Z",
     "iopub.status.idle": "2023-08-18T07:01:43.953195Z",
     "shell.execute_reply": "2023-08-18T07:01:43.952092Z"
    },
    "origin_pos": 77,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166972e",
   "metadata": {
    "origin_pos": 80
   },
   "source": [
    "指定`axis=1`將透過匯總所有列的元素降維（軸1）。因此，輸入軸1的維數在輸出形狀中消失。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50e59a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.958180Z",
     "iopub.status.busy": "2023-08-18T07:01:43.957431Z",
     "iopub.status.idle": "2023-08-18T07:01:43.965338Z",
     "shell.execute_reply": "2023-08-18T07:01:43.964267Z"
    },
    "origin_pos": 82,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6fac47",
   "metadata": {
    "origin_pos": 85
   },
   "source": [
    "沿著行和列對矩陣求和，等價於對矩陣的所有元素進行求和。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1ba976a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.970587Z",
     "iopub.status.busy": "2023-08-18T07:01:43.969706Z",
     "iopub.status.idle": "2023-08-18T07:01:43.977405Z",
     "shell.execute_reply": "2023-08-18T07:01:43.976340Z"
    },
    "origin_pos": 87,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1])  # 结果和A.sum()相同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38057dab",
   "metadata": {
    "origin_pos": 90
   },
   "source": [
    "[**一個與求和相關的量是*平均值*（mean或average）**]。\n",
    "我們透過將總和除以元素總數來計算平均值。\n",
    "在程式碼中，我們可以呼叫函數來計算任意形狀張量的平均值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d901892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.982674Z",
     "iopub.status.busy": "2023-08-18T07:01:43.981742Z",
     "iopub.status.idle": "2023-08-18T07:01:43.990067Z",
     "shell.execute_reply": "2023-08-18T07:01:43.988981Z"
    },
    "origin_pos": 92,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d311917",
   "metadata": {
    "origin_pos": 95
   },
   "source": [
    "同樣地，計算平均值的函數也可以沿指定軸降低張量的維度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65c39834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:43.995223Z",
     "iopub.status.busy": "2023-08-18T07:01:43.994254Z",
     "iopub.status.idle": "2023-08-18T07:01:44.003242Z",
     "shell.execute_reply": "2023-08-18T07:01:44.002162Z"
    },
    "origin_pos": 97,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24d4e3",
   "metadata": {
    "origin_pos": 100
   },
   "source": [
    "### 非降維求和\n",
    " \n",
    ":label:`subseq_lin-alg-non-reduction`\n",
    " \n",
    "然而，有時在呼叫函數來[**計算總和或平均值時保持軸數不變**]會很有用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cc17274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.008471Z",
     "iopub.status.busy": "2023-08-18T07:01:44.007568Z",
     "iopub.status.idle": "2023-08-18T07:01:44.016007Z",
     "shell.execute_reply": "2023-08-18T07:01:44.014845Z"
    },
    "origin_pos": 102,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae08c05",
   "metadata": {
    "origin_pos": 105
   },
   "source": [
    "例如，由於`sum_A`在對每列進行求和後仍保持兩個軸，我們可以(**透過廣播將`A`除以`sum_A`**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63a5b49d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.020992Z",
     "iopub.status.busy": "2023-08-18T07:01:44.020591Z",
     "iopub.status.idle": "2023-08-18T07:01:44.028726Z",
     "shell.execute_reply": "2023-08-18T07:01:44.027663Z"
    },
    "origin_pos": 107,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2a480",
   "metadata": {
    "origin_pos": 110
   },
   "source": [
    "如果我們想沿[**某個軸計算`A`元素的累積總和**]，\n",
    "比如`axis=0`（按行計算），可以呼叫`cumsum`函數。\n",
    "此函數不會沿任何軸降低輸入張量的維度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27eb9655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.033849Z",
     "iopub.status.busy": "2023-08-18T07:01:44.033115Z",
     "iopub.status.idle": "2023-08-18T07:01:44.041281Z",
     "shell.execute_reply": "2023-08-18T07:01:44.040150Z"
    },
    "origin_pos": 112,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dec732",
   "metadata": {
    "origin_pos": 115
   },
   "source": [
    "## 點積（Dot Product）\n",
    "\n",
    "我們已經學習了按元素操作、求和及平均值。\n",
    "另一個最基本的操作之一是點積。\n",
    "給定兩個向量$\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$，\n",
    "它們的*點積*（dot product）$\\mathbf{x}^\\top\\mathbf{y}$\n",
    "（或$\\langle\\mathbf{x},\\mathbf{y}\\rangle$）\n",
    "是相同位置的按元素乘積的和：$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$。\n",
    " \n",
    "[~~點積是相同位置的按元素乘積的和~~]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7840d740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.045914Z",
     "iopub.status.busy": "2023-08-18T07:01:44.045514Z",
     "iopub.status.idle": "2023-08-18T07:01:44.058183Z",
     "shell.execute_reply": "2023-08-18T07:01:44.057040Z"
    },
    "origin_pos": 117,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18953e09",
   "metadata": {
    "origin_pos": 120
   },
   "source": [
    "注意，(**我們可以透過執行按元素乘法，然後進行求和來表示兩個向量的點積**)：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dadc2a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.062812Z",
     "iopub.status.busy": "2023-08-18T07:01:44.062422Z",
     "iopub.status.idle": "2023-08-18T07:01:44.070070Z",
     "shell.execute_reply": "2023-08-18T07:01:44.068907Z"
    },
    "origin_pos": 122,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4b767",
   "metadata": {
    "origin_pos": 125
   },
   "source": [
    "點積在很多場合都很有用。\n",
    "例如，給定一組由向量$\\mathbf{x} \\in \\mathbb{R}^d$表示的值，\n",
    "和一組由$\\mathbf{w} \\in \\mathbb{R}^d$表示的權重。\n",
    "$\\mathbf{x}$中的值根據權重$\\mathbf{w}$的加權和，\n",
    "可以表示為點積$\\mathbf{x}^\\top \\mathbf{w}$。\n",
    "當權重為非負數且和為1（即$\\left(\\sum_{i=1}^{d}{w_i}=1\\right)$）時，\n",
    "點積表示*加權平均*（weighted average）。\n",
    "將兩個向量規範化得到單位長度後，點積表示它們夾角的餘弦。\n",
    "本節後面的內容將正式介紹*長度*（length）的概念。\n",
    "\n",
    "## 矩陣-向量積\n",
    "\n",
    "現在我們知道如何計算點積，可以開始理解*矩陣-向量積*（matrix-vector product）。\n",
    "回顧分別在 :eqref:`eq_matrix_def`和 :eqref:`eq_vec_def`中定義的矩陣$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$和向量$\\mathbf{x} \\in \\mathbb{R}^n$。\n",
    "讓我們將矩陣$\\mathbf{A}$用它的行向量表示：\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "其中每個$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$都是行向量，表示矩陣的第$i$行。\n",
    "[**矩陣向量積$\\mathbf{A}\\mathbf{x}$是一個長度為$m$的列向量，\n",
    "其第$i$個元素是點積$\\mathbf{a}^\\top_i \\mathbf{x}$**]：\n",
    " \n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "我們可以把一個矩陣$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$乘法看作一個從$\\mathbb{R}^{n}$到$\\mathbb{R}^{m}$向量的轉換。\n",
    "這些轉換是非常有用的，例如可以用方陣的乘法來表示旋轉。\n",
    "後續章節將講到，我們也可以使用矩陣-向量積來描述在給定前一層的值時，\n",
    "求解神經網路每一層所需的複雜計算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff356a",
   "metadata": {
    "origin_pos": 127,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在程式碼中使用張量表示矩陣-向量積，我們使用`mv`函數。\n",
    "當我們為矩陣`A`和向量`x`調用`torch.mv(A, x)`時，會執行矩陣-向量積。\n",
    "注意，`A`的列維數（沿軸1的長度）必須與`x`的維數（其長度）相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62c6809c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.075294Z",
     "iopub.status.busy": "2023-08-18T07:01:44.074579Z",
     "iopub.status.idle": "2023-08-18T07:01:44.082607Z",
     "shell.execute_reply": "2023-08-18T07:01:44.081496Z"
    },
    "origin_pos": 130,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d1be1",
   "metadata": {
    "origin_pos": 133
   },
   "source": [
    "## 矩陣-矩陣乘法\n",
    " \n",
    "在掌握點積和矩陣-向量積的知識後，\n",
    "那麼**矩陣-矩陣乘法**（matrix-matrix multiplication）應該很簡單。\n",
    " \n",
    "假設有兩個矩陣$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$和$\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$：\n",
    " \n",
    "$$\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "用行向量$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$表示矩陣$\\mathbf{A}$的第$i$行，並讓列向量$\\mathbf{b}_{j} \\in \\mathbb{R}^k$作為矩陣$\\mathbf{B}$的第$j$列。要生成矩陣積$\\mathbf{C} = \\mathbf{A}\\mathbf{B}$，最簡單的方法是考慮$\\mathbf{A}$的行向量和$\\mathbf{B}$的列向量:\n",
    " \n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "當我們簡單地將每個元素$c_{ij}$計算為點積$\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
    " \n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "[**我們可以將矩陣-矩陣乘法$\\mathbf{AB}$看作簡單地執行$m$次矩陣-向量積，並將結果拼接在一起，形成一個$n \\times m$矩陣**]。\n",
    "在下面的程式碼中，我們在`A`和`B`上執行矩陣乘法。\n",
    "這裡的`A`是一個5行4列的矩陣，`B`是一個4行3列的矩陣。\n",
    "兩者相乘後，我們得到了一個5行3列的矩陣。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e3efc16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.087651Z",
     "iopub.status.busy": "2023-08-18T07:01:44.086870Z",
     "iopub.status.idle": "2023-08-18T07:01:44.095375Z",
     "shell.execute_reply": "2023-08-18T07:01:44.094329Z"
    },
    "origin_pos": 135,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab0ddd",
   "metadata": {
    "origin_pos": 138
   },
   "source": [
    "矩陣-矩陣乘法可以簡單地稱為**矩陣乘法**，不應與「Hadamard積」混淆。\n",
    " \n",
    "## 範數\n",
    ":label:`subsec_lin-algebra-norms`\n",
    " \n",
    "線性代數中最有用的一些運算符是*範數*（norm）。\n",
    "非正式地說，向量的*範數*是表示一個向量有多大。\n",
    "這裡考慮的*大小*（size）概念不涉及維度，而是分量的大小。\n",
    "\n",
    "在線性代數中，向量範數是將向量映射到標量的函數$f$。\n",
    "給定任意向量$\\mathbf{x}$，向量範數要滿足一些屬性。\n",
    "第一個性質是：如果我們按常數因子$\\alpha$縮放向量的所有元素，\n",
    "其範數也會按相同常數因子的*絕對值*縮放：\n",
    "\n",
    "$$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$$\n",
    "\n",
    "第二個性質是熟悉的三角不等式:\n",
    "\n",
    "$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$$\n",
    "\n",
    "第三個性質簡單地說範數必須是非負的:\n",
    "\n",
    "$$f(\\mathbf{x}) \\geq 0.$$\n",
    "\n",
    "這是有道理的。因為在大多數情況下，任何東西的最小的*大小*是0。\n",
    "最後一個性質要求範數最小為0，當且僅當向量全由0組成。\n",
    "\n",
    "$$\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$$\n",
    "\n",
    "範數聽起來很像距離的度量。\n",
    "歐幾里得距離和畢達哥拉斯定理中的非負性概念和三角不等式可能會給出一些啟發。\n",
    "事實上，歐幾里得距離是一個$L_2$範數：\n",
    "假設$n$維向量$\\mathbf{x}$中的元素是$x_1,\\ldots,x_n$，其[**$L_2$*範數*是向量元素平方和的平方根：**]\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$**)\n",
    "\n",
    "其中，在$L_2$範數中常常省略下標$2$，也就是說$\\|\\mathbf{x}\\|$等同於$\\|\\mathbf{x}\\|_2$。\n",
    "在程式碼中，我們可以按如下方式計算向量的$L_2$範數。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f829c100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.100377Z",
     "iopub.status.busy": "2023-08-18T07:01:44.099628Z",
     "iopub.status.idle": "2023-08-18T07:01:44.107745Z",
     "shell.execute_reply": "2023-08-18T07:01:44.106642Z"
    },
    "origin_pos": 140,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9608c4c",
   "metadata": {
    "origin_pos": 143
   },
   "source": [
    "深度學習中更經常地使用$L_2$範數的平方，也會經常遇到[**$L_1$範數，它表示為向量元素的絕對值之和：**]\n",
    " \n",
    "(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n",
    " \n",
    "與$L_2$範數相比，$L_1$範數受異常值的影響較小。\n",
    "為了計算$L_1$範數，我們將絕對值函數和按元素求和組合起來。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01356584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.143775Z",
     "iopub.status.busy": "2023-08-18T07:01:44.142900Z",
     "iopub.status.idle": "2023-08-18T07:01:44.151418Z",
     "shell.execute_reply": "2023-08-18T07:01:44.150335Z"
    },
    "origin_pos": 145,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9454ae0",
   "metadata": {
    "origin_pos": 148
   },
   "source": [
    "$L_2$範數和$L_1$範數都是更一般的$L_p$範數的特例：\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n",
    " \n",
    "類似於向量的$L_2$範數，[**矩陣**]$\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$(**的*Frobenius範數*（Frobenius norm）是矩陣元素平方和的平方根：**)\n",
    "\n",
    "(**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**)\n",
    " \n",
    "Frobenius範數滿足向量範數的所有性質，它就像是矩陣形向量的$L_2$範數。\n",
    "調用以下函數將計算矩陣的Frobenius範數。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a8792ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:44.156452Z",
     "iopub.status.busy": "2023-08-18T07:01:44.155694Z",
     "iopub.status.idle": "2023-08-18T07:01:44.163608Z",
     "shell.execute_reply": "2023-08-18T07:01:44.162540Z"
    },
    "origin_pos": 150,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7470df",
   "metadata": {
    "origin_pos": 153
   },
   "source": [
    "### 範數和目標\n",
    " \n",
    ":label:`subsec_norms_and_objectives`\n",
    " \n",
    "在深度學習中，我們經常試圖解決優化問題：\n",
    "*最大化*分配給觀測數據的概率；\n",
    "*最小化*預測和真實觀測之間的距離。\n",
    "用向量表示物品（如單詞、產品或新聞文章），以便最小化相似項目之間的距離，最大化不同項目之間的距離。\n",
    "目標，或許是深度學習算法最重要的組成部分（除了數據），通常被表達為範數。\n",
    "\n",
    "## 關於線性代數的更多信息\n",
    "\n",
    "僅用一節，我們就教會了閱讀本書所需的、用以理解現代深度學習的線性代數。\n",
    "線性代數還有很多，其中很多數學對於機器學習非常有用。\n",
    "例如，矩陣可以分解為因子，這些分解可以顯示真實世界數據集中的低維結構。\n",
    "機器學習的整個子領域都側重於使用矩陣分解及其向高階張量的泛化，來發現數據集中的結構並解決預測問題。\n",
    "當開始動手嘗試並在真實數據集上應用了有效的機器學習模型，你會更傾向於學習更多數學。\n",
    "因此，這一節到此結束，本書將在後面介紹更多數學知識。\n",
    "\n",
    "如果渴望了解有關線性代數的更多信息，可以參考[線性代數運算的在線附錄](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html)或其他優秀資源 :cite:`Strang.1993,Kolter.2008,Petersen.Pedersen.ea.2008`。\n",
    "\n",
    "## 小結\n",
    "\n",
    "* 標量、向量、矩陣和張量是線性代數中的基本數學對象。\n",
    "* 向量泛化自標量，矩陣泛化自向量。\n",
    "* 標量、向量、矩陣和張量分別具有零、一、二和任意數量的軸。\n",
    "* 一個張量可以通過`sum`和`mean`沿指定的軸降低維度。\n",
    "* 兩個矩陣的按元素乘法被稱為他們的Hadamard積。它與矩陣乘法不同。\n",
    "* 在深度學習中，我們經常使用範數，如$L_1$範數、$L_2$範數和Frobenius範數。\n",
    "* 我們可以對標量、向量、矩陣和張量執行各種操作。\n",
    "\n",
    "## 練習\n",
    "\n",
    "1. 證明一個矩陣$\\mathbf{A}$的轉置的轉置是$\\mathbf{A}$，即$(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$。\n",
    "1. 給出兩個矩陣$\\mathbf{A}$和$\\mathbf{B}$，證明\"它們轉置的和\"等於\"它們和的轉置\"，即$\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$。\n",
    "1. 給定任意方陣$\\mathbf{A}$，$\\mathbf{A} + \\mathbf{A}^\\top$總是對稱的嗎？為什麼？\n",
    "1. 本節中定義了形狀$(2,3,4)$的張量`X`。`len(X)`的輸出結果是什麼？\n",
    "1. 對於任意形狀的張量`X`，`len(X)`是否總是對應於`X`特定軸的長度？這個軸是什麼？\n",
    "1. 運行`A/A.sum(axis=1)`，看看會發生什麼。請分析一下原因？\n",
    "1. 考慮一個具有形狀$(2,3,4)$的張量，在軸0、1、2上的求和輸出是什麼形狀？\n",
    "1. 為`linalg.norm`函數提供3個或更多軸的張量，並觀察其輸出。對於任意形狀的張量這個函數計算得到什麼？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6f271",
   "metadata": {
    "origin_pos": 155,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1751)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f081f",
   "metadata": {},
   "source": [
    "## 我的練習回答，不一定對\n",
    "\n",
    "**1. 證明一個矩陣 $\\mathbf{A}$ 的轉置的轉置是 $\\mathbf{A}$，即 $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$。**\n",
    "\n",
    "### 解答\n",
    "轉置操作的定義是將矩陣的行與列交換。例如，對於矩陣 $\\mathbf{A}$，如果第 $\\ i $ 行、第 $\\ j $列的元素為 $\\ a_{ij} $，則轉置後的矩陣 $\\mathbf{A}^\\top $ 的第 $\\ j $ 行、第 $\\ i $ 列的元素仍然是 $\\ a_{ij} $。\n",
    "\n",
    "當我們再對 $\\mathbf{A}^\\top $ 進行一次轉置操作時：\n",
    "1. 第 $\\ j $ 行、第 $\\ i $ 列的元素會變回原矩陣的第 $\\ i $ 行、第 $\\ j $ 列。\n",
    "2. 結果等於原矩陣 $\\mathbf{A} $。\n",
    "\n",
    "因此，數學上可表示為：\n",
    "$\\\n",
    "( \\mathbf{A}^\\top )^\\top = \\mathbf{A}\n",
    "\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca4b98fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定義一個矩陣 A\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 計算雙重轉置\n",
    "A_double_transpose = A.T.T\n",
    "\n",
    "# 驗證是否等於原矩陣\n",
    "print(torch.equal(A, A_double_transpose))  # 應輸出 True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc3291",
   "metadata": {},
   "source": [
    "### 練習 2 解答\n",
    "\n",
    "給出兩個矩陣 $\\mathbf{A}$ 和 $\\mathbf{B}$，我們需要證明：\n",
    "\n",
    "$\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$\n",
    "\n",
    "#### 證明過程\n",
    "1. 轉置操作是將矩陣的行與列交換。\n",
    "2. $\\mathbf{A}^\\top + \\mathbf{B}^\\top$ 是先分別將 $\\mathbf{A}$ 和 $\\mathbf{B}$ 轉置後相加。\n",
    "3. $(\\mathbf{A} + \\mathbf{B})^\\top$ 是先將 $\\mathbf{A}$ 和 $\\mathbf{B}$ 相加後再轉置。\n",
    "4. 由於矩陣加法是按元素進行的，兩者等價。\n",
    "\n",
    "因此，數學上可表示為：\n",
    "$\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$\n",
    "\n",
    "#### 程式驗證\n",
    "以下程式碼驗證上述等式是否成立：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b11bcc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定義兩個矩陣 A 和 B\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "B = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# 計算轉置的和與和的轉置\n",
    "lhs = A.T + B.T  # 左邊: A^T + B^T\n",
    "rhs = (A + B).T  # 右邊: (A + B)^T\n",
    "\n",
    "# 驗證兩者是否相等\n",
    "print(torch.equal(lhs, rhs))  # 應輸出 True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce98845",
   "metadata": {},
   "source": [
    "## 練習三題目\n",
    "\n",
    "**3. 給定任意方陣 $\\mathbf{A}$，$\\mathbf{A} + \\mathbf{A}^\\top$ 總是對稱的嗎？為什麼？**\n",
    "\n",
    "---\n",
    "\n",
    "### 解答\n",
    "\n",
    "是的，對於任意方陣 $\\mathbf{A}$，$\\mathbf{A} + \\mathbf{A}^\\top$ 總是對稱的。\n",
    "\n",
    "#### 證明\n",
    "1. 對稱矩陣的定義是 $\\mathbf{B}^\\top = \\mathbf{B}$。\n",
    "2. 設 $\\mathbf{B} = \\mathbf{A} + \\mathbf{A}^\\top$，則：\n",
    "   $$\n",
    "   \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{A}^\\top)^\\top\n",
    "   $$\n",
    "3. 根據矩陣轉置的性質，轉置的和等於和的轉置：\n",
    "   $$\n",
    "   \\mathbf{B}^\\top = \\mathbf{A}^\\top + (\\mathbf{A}^\\top)^\\top\n",
    "   $$\n",
    "4. 又因為 $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$，所以：\n",
    "   $$\n",
    "   \\mathbf{B}^\\top = \\mathbf{A} + \\mathbf{A}^\\top = \\mathbf{B}\n",
    "   $$\n",
    "5. 因此，$\\mathbf{B}$ 是對稱矩陣。\n",
    "---\n",
    "\n",
    "### 程式驗證結果\n",
    "該程式碼輸出 `True`，證明 $\\mathbf{A} + \\mathbf{A}^\\top$ 總是對稱矩陣。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a1bdda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定義一個隨機方陣 A\n",
    "A = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "\n",
    "# 計算 A + A^T\n",
    "B = A + A.T\n",
    "\n",
    "# 驗證 B 是否對稱\n",
    "print(torch.equal(B, B.T))  # 應輸出 True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d30ddc",
   "metadata": {},
   "source": [
    "### 練習四題目\n",
    " \n",
    "**4. 本節中定義了形狀 $(2, 3, 4)$ 的張量 $X$。$\\text{len}(X)$ 的輸出結果是什麼？**\n",
    "\n",
    "---\n",
    " \n",
    "### 解答\n",
    " \n",
    "$\\text{len}(X)$ 的結果是張量 $X$ 在 **最外層維度的大小**，即它的第 $0$ 軸的大小。\n",
    " \n",
    "在本題中，$X$ 的形狀為 $(2, 3, 4)$，最外層維度大小為 $2$，因此 $\\text{len}(X)$ 的輸出結果是：\n",
    " \n",
    "$$\n",
    "\\text{len}(X) = 2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 程式驗證\n",
    "\n",
    "以下程式碼驗證上述結論：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 定義形狀為 (2, 3, 4) 的張量 X\n",
    "X = torch.zeros((2, 3, 4))\n",
    "\n",
    "# 輸出 len(X)\n",
    "print(len(X))  # 應輸出 2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 結果\n",
    "\n",
    "該程式碼輸出 `2`，證明 `len(X)` 確實對應於張量 `X` 的最外層維度大小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b4598c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定義形狀為 (2, 3, 4) 的張量 X\n",
    "X = torch.zeros((2, 3, 4))\n",
    "\n",
    "# 輸出 len(X)\n",
    "print(len(X))  # 應輸出 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af23211",
   "metadata": {},
   "source": [
    "### 練習五題目\n",
    "\n",
    "**5. 對於任意形狀的張量 `X`，`len(X)` 是否總是對應於 `X` 特定軸的長度？這個軸是什麼？**\n",
    "\n",
    "---\n",
    "\n",
    "### 解答\n",
    "\n",
    "是的，對於任意形狀的張量 `X`，`len(X)` 總是對應於 **張量 `X` 的第 0 軸（最外層維度）的長度**。\n",
    "\n",
    "#### 理由\n",
    "1. 在 Python 中，`len()` 函數返回一維容器的元素數量。\n",
    "2. 對於多維張量，`len(X)` 返回的是張量第 0 軸的大小，即最外層包含的元素數量。\n",
    "3. 無論 `X` 的形狀為 `(a, b, c, ...)`，`len(X)` 始終返回 `a`。\n",
    "\n",
    "---\n",
    "\n",
    "### 程式驗證\n",
    "\n",
    "以下程式碼展示不同形狀的張量 `X`，並驗證 `len(X)` 的行為：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 定義不同形狀的張量\n",
    "X1 = torch.zeros((2, 3, 4))  # 形狀 (2, 3, 4)\n",
    "X2 = torch.zeros((5, 6))     # 形狀 (5, 6)\n",
    "X3 = torch.zeros((7,))       # 形狀 (7,)\n",
    "\n",
    "# 輸出 len(X)\n",
    "print(len(X1))  # 應輸出 2\n",
    "print(len(X2))  # 應輸出 5\n",
    "print(len(X3))  # 應輸出 7\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 結果\n",
    "\n",
    "1. 張量形狀為 `(2, 3, 4)` 時，`len(X1) = 2`。\n",
    "2. 張量形狀為 `(5, 6)` 時，`len(X2) = 5`。\n",
    "3. 張量形狀為 `(7,)` 時，`len(X3) = 7`。\n",
    "\n",
    "結論：`len(X)` 總是對應於張量 `X` 的第 0 軸的長度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bee15823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定義不同形狀的張量\n",
    "X1 = torch.zeros((2, 3, 4))  # 形狀 (2, 3, 4)\n",
    "X2 = torch.zeros((5, 6))     # 形狀 (5, 6)\n",
    "X3 = torch.zeros((7,))       # 形狀 (7,)\n",
    "\n",
    "# 輸出 len(X)\n",
    "print(len(X1))  # 應輸出 2\n",
    "print(len(X2))  # 應輸出 5\n",
    "print(len(X3))  # 應輸出 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a08a50",
   "metadata": {},
   "source": [
    "### 練習六題目\n",
    "\n",
    "**6. 運行 `A / A.sum(axis=1)`，看看會發生什麼。請分析一下原因？**\n",
    "\n",
    "---\n",
    "\n",
    "### 解答\n",
    "\n",
    "運行 `A / A.sum(axis=1)` 的結果取決於張量 `A` 的形狀和計算方式。以下是分析過程：\n",
    "\n",
    "1. **計算步驟**：\n",
    "   - `A.sum(axis=1)` 計算 `A` 在第 1 軸上的元素和，輸出形狀為 `(n, )`，其中 `n` 是第 0 軸的大小。\n",
    "   - `A / A.sum(axis=1)` 是逐元素除法，其中 `A` 和 `A.sum(axis=1)` 必須進行 **廣播**。\n",
    "\n",
    "2. **廣播機制**：\n",
    "   - 當 `A` 是一個二維張量（形狀為 `(n, m)`）時，`A.sum(axis=1)` 的形狀為 `(n,)`。\n",
    "   - 廣播機制將 `(n,)` 擴展為 `(n, 1)`，以便與 `A` 匹配。\n",
    "   - 最終，每行的元素會被該行的總和除以，進行逐元素除法。\n",
    "\n",
    "---\n",
    "\n",
    "### 程式驗證\n",
    "\n",
    "以下程式碼展示此操作的行為和結果：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 定義矩陣 A\n",
    "A = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "\n",
    "# 計算 A / A.sum(axis=1)\n",
    "result = A / A.sum(axis=1, keepdim=True)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"A:\")\n",
    "print(A)\n",
    "print(\"\\nA.sum(axis=1):\")\n",
    "print(A.sum(axis=1, keepdim=True))\n",
    "print(\"\\nA / A.sum(axis=1):\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 結果\n",
    "\n",
    "1. 原矩陣 $A$：\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   1 & 2 & 3 \\\\\n",
    "   4 & 5 & 6\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. 每行總和：\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   6 \\\\\n",
    "   15\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. 逐元素相除：\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   \\frac{1}{6} & \\frac{2}{6} & \\frac{3}{6} \\\\\n",
    "   \\frac{4}{15} & \\frac{5}{15} & \\frac{6}{15}\n",
    "   \\end{bmatrix}\n",
    "   =\n",
    "   \\begin{bmatrix}\n",
    "   0.1667 & 0.3333 & 0.5 \\\\\n",
    "   0.2667 & 0.3333 & 0.4\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 分析原因\n",
    "\n",
    "這一操作將每行元素歸一化（Normalize），使得每行的元素總和等於 1。這是因為：\n",
    "$$\n",
    "\\frac{\\text{每個元素}}{\\text{該行總和}}\n",
    "$$\n",
    "保證比例相等，常用於數據標準化或機率分佈計算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b099fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "A.sum(axis=1):\n",
      "tensor([[ 6.],\n",
      "        [15.]])\n",
      "\n",
      "A / A.sum(axis=1):\n",
      "tensor([[0.1667, 0.3333, 0.5000],\n",
      "        [0.2667, 0.3333, 0.4000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定義矩陣 A\n",
    "A = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "\n",
    "# 計算 A / A.sum(axis=1)\n",
    "result = A / A.sum(axis=1, keepdim=True)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"A:\")\n",
    "print(A)\n",
    "print(\"\\nA.sum(axis=1):\")\n",
    "print(A.sum(axis=1, keepdim=True))\n",
    "print(\"\\nA / A.sum(axis=1):\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e525e3",
   "metadata": {},
   "source": [
    "### 練習七題目\n",
    "\n",
    "**7. 考慮一個具有形狀 \\( (2, 3, 4) \\) 的張量，在軸 0、1、2 上的求和輸出是什麼形狀？**\n",
    "\n",
    "---\n",
    "\n",
    "### 解答\n",
    "\n",
    "#### 分析\n",
    "\n",
    "1. 原始張量的形狀為 \\( (2, 3, 4) \\)，表示：\n",
    "   - 軸 0 有大小為 2 的元素。\n",
    "   - 軸 1 有大小為 3 的元素。\n",
    "   - 軸 2 有大小為 4 的元素。\n",
    "\n",
    "2. 求和操作的結果：\n",
    "   - **沿軸 0 求和**：輸出形狀為 \\( (3, 4) \\)，因為軸 0 的大小被壓縮掉。\n",
    "   - **沿軸 1 求和**：輸出形狀為 \\( (2, 4) \\)，因為軸 1 的大小被壓縮掉。\n",
    "   - **沿軸 2 求和**：輸出形狀為 \\( (2, 3) \\)，因為軸 2 的大小被壓縮掉。\n",
    "   - **沿所有軸求和**：輸出形狀為標量（即單一值），因為所有維度都被壓縮掉。\n",
    "\n",
    "---\n",
    "\n",
    "### 程式驗證\n",
    "\n",
    "以下程式碼展示張量在不同軸上的求和操作及結果：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 定義形狀為 (2, 3, 4) 的張量\n",
    "X = torch.arange(24, dtype=torch.float32).reshape((2, 3, 4))\n",
    "\n",
    "# 沿不同軸求和\n",
    "sum_axis_0 = X.sum(axis=0)\n",
    "sum_axis_1 = X.sum(axis=1)\n",
    "sum_axis_2 = X.sum(axis=2)\n",
    "sum_all = X.sum()\n",
    "\n",
    "# 輸出結果及形狀\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"\\nSum along axis 0 (shape):\", sum_axis_0.shape)\n",
    "print(sum_axis_0)\n",
    "print(\"\\nSum along axis 1 (shape):\", sum_axis_1.shape)\n",
    "print(sum_axis_1)\n",
    "print(\"\\nSum along axis 2 (shape):\", sum_axis_2.shape)\n",
    "print(sum_axis_2)\n",
    "print(\"\\nSum along all axes (shape):\", sum_all.shape)\n",
    "print(sum_all)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 結果\n",
    "\n",
    "假設 \\( X \\) 是從 0 到 23 的張量，程式輸出的結果如下：\n",
    "\n",
    "1. **沿軸 0 求和（形狀 \\( (3, 4) \\)）**：\n",
    "   \\[\n",
    "   \\begin{bmatrix}\n",
    "   12 & 14 & 16 & 18 \\\\\n",
    "   20 & 22 & 24 & 26 \\\\\n",
    "   28 & 30 & 32 & 34\n",
    "   \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "2. **沿軸 1 求和（形狀 \\( (2, 4) \\)）**：\n",
    "   \\[\n",
    "   \\begin{bmatrix}\n",
    "   12 & 15 & 18 & 21 \\\\\n",
    "   48 & 51 & 54 & 57\n",
    "   \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "3. **沿軸 2 求和（形狀 \\( (2, 3) \\)）**：\n",
    "   \\[\n",
    "   \\begin{bmatrix}\n",
    "   6 & 22 & 38 \\\\\n",
    "   54 & 70 & 86\n",
    "   \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "4. **沿所有軸求和（標量值）**：\n",
    "   \\[\n",
    "   276\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### 總結\n",
    "\n",
    "- 沿軸 0 求和：形狀 \\( (3, 4) \\)。\n",
    "- 沿軸 1 求和：形狀 \\( (2, 4) \\)。\n",
    "- 沿軸 2 求和：形狀 \\( (2, 3) \\)。\n",
    "- 沿所有軸求和：標量值。\n",
    "\n",
    "這說明張量在每個軸上的求和操作會壓縮該軸並保留其他軸的形狀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa34c2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: torch.Size([2, 3, 4])\n",
      "\n",
      "Sum along axis 0 (shape): torch.Size([3, 4])\n",
      "tensor([[12., 14., 16., 18.],\n",
      "        [20., 22., 24., 26.],\n",
      "        [28., 30., 32., 34.]])\n",
      "\n",
      "Sum along axis 1 (shape): torch.Size([2, 4])\n",
      "tensor([[12., 15., 18., 21.],\n",
      "        [48., 51., 54., 57.]])\n",
      "\n",
      "Sum along axis 2 (shape): torch.Size([2, 3])\n",
      "tensor([[ 6., 22., 38.],\n",
      "        [54., 70., 86.]])\n",
      "\n",
      "Sum along all axes (shape): torch.Size([])\n",
      "tensor(276.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定義形狀為 (2, 3, 4) 的張量\n",
    "X = torch.arange(24, dtype=torch.float32).reshape((2, 3, 4))\n",
    "\n",
    "# 沿不同軸求和\n",
    "sum_axis_0 = X.sum(axis=0)\n",
    "sum_axis_1 = X.sum(axis=1)\n",
    "sum_axis_2 = X.sum(axis=2)\n",
    "sum_all = X.sum()\n",
    "\n",
    "# 輸出結果及形狀\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"\\nSum along axis 0 (shape):\", sum_axis_0.shape)\n",
    "print(sum_axis_0)\n",
    "print(\"\\nSum along axis 1 (shape):\", sum_axis_1.shape)\n",
    "print(sum_axis_1)\n",
    "print(\"\\nSum along axis 2 (shape):\", sum_axis_2.shape)\n",
    "print(sum_axis_2)\n",
    "print(\"\\nSum along all axes (shape):\", sum_all.shape)\n",
    "print(sum_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
