
這只是論文解讀心得，假如您需要更深入的解讀以及研究，請至[原文的網頁](https://arxiv.org/pdf/2408.00714v1)以及[程式碼的項目網址](https://github.com/facebookresearch/segment-anything-2)。

## SAM 2：影像與影片中的任意分割


Meta FAIR

本文在此介紹 Segment Anything Model 2（SAM 2），這是一個基礎模型，旨在解決影像與影片中的可提示視覺分割問題。文章構建了一個數據引擎，透過用戶互動來改善模型和數據，並收集了迄今為止最大規模的影片分割數據集。文章的模型採用了一個簡單的轉換器架構，並具備串流功能，可實現即時的影片處理。使用 SAM 2 訓練的數據在多樣的任務中展現出強勁的性能。

在影片分割方面，文章觀察到該模型比傳統方法具有更好的準確性，並且所需的互動次數少了三倍。在影像分割方面，模型比起 SAM（Segment Anything Model）更為準確，且速度快了六倍。文章相信這些數據、模型及見解將成為影片分割及相關感知任務的重要里程碑。

文章已經釋出了模型版本、數據集及一個互動演示。

### 相關連結：
- **Demo**: [https://sam2.metademolab.com](https://sam2.metademolab.com)
- **Code**: [https://github.com/facebookresearch/segment-anything-2](https://github.com/facebookresearch/segment-anything-2)
- **Website**: [https://ai.meta.com/sam2](https://ai.meta.com/sam2)

## 1. 引言

在引言部分，作者介紹了 **Segment Anything Model 2 (SAM 2)**，這是一個統一的模型，能夠同時應用於影像和影片的分割，進行 **提示式視覺分割** 任務。這項技術可以根據用戶的提示（如點、框或遮罩）來分割影片中的物體，並能夠通過一種串流記憶機制記住物體在影片中的前幀信息，從而提高分割的準確性。

他們使用了一個 **數據引擎** 來生成訓練數據，並且通過與人工標註者的交互，不斷優化這些數據。最終生成了 **Segment Anything Video (SA-V)** 數據集，這是目前最大規模的影片分割數據集，包含 50.9K 部影片與 35.5M 個遮罩，數據生成速度比現有方法快了 8.4 倍。

在實驗中，SAM 2 不僅展示了在影片分割中的優越表現，還比先前的 SAM 模型快了 6 倍，並且使用更少的提示次數即可達到更好的分割準確度。

這些結果表明，SAM 2 是一個顯著的進步，不僅能處理影像，還能有效應用於影片的分割任務。最後，作者釋出了模型版本、數據集及互動演示供研究與應用。

## 2. 相關工作

### 影像分割
Segment Anything (SA) 是一個提示式的影像分割模型，其目的是在給定提示（如框選或感興趣物體的點）後輸出有效的分割遮罩。SA 模型基於 SA-1B 資料集進行訓練，允許零樣本分割並支援靈活的提示，這使得它能被應用於多種下游應用。近期的研究則著重於提升 SA 的分割質量。例如，HQ-SAM 引入了高質量的輸出符號並在細粒度遮罩上進行訓練，而其他如 EfficientSAM、MobileSAM 和 FastSAM 則側重於提升 SAM 的效率，使其能夠更廣泛應用於實際應用中，如醫學影像、遙感、運動分割及偽裝物體檢測等。

### 互動式影片物體分割 (iVOS)
互動式影片物體分割已經成為一個關鍵的任務，用戶可以通過塗鴉、點擊或框選來有效獲得影片中的物體分割（遮罩）。一些早期的方法通過圖形優化來指導分割註釋過程，而近期的研究則採用模組化設計，將用戶輸入轉換為單幀中的遮罩，並將其推廣到其他幀。這些方法通常結合互動式標註，讓用戶能夠通過提示不斷修正分割結果。

DAVIS 互動基準允許用戶在多幀上對物體進行分割，而 SAM 2 則擴展了這一基準，並且能夠在提示式視覺分割任務中運行。基於點擊的輸入也被證實更容易進行收集，且 SAM 在與影像追蹤器結合時表現出色。

### 半監督式影片物體分割 (VOS)
半監督式影片物體分割通常要求在第一幀中提供遮罩，並且必須準確追蹤該物體至後續幀。這個任務強調了遮罩在整段影片中的一致性，並且在許多應用中，例如影片編輯、機器人技術、自動背景移除等，都具有很大的應用價值。近年的研究也提出了一些新方法來提升分割效果，例如使用深度學習模組來處理多幀訊息，進行更快的推斷。

### 影片分割數據集
許多數據集被提出來支持影片分割任務。早期的數據集如 DAVIS 提供了高質量的註釋，但受限於其規模，無法支援深度學習的方法。YouTube-VOS 是首個大規模的影片分割數據集，涵蓋了 94 個物體類別，4,000 多段影片。隨著算法的性能趨於飽和，研究者開始提升任務難度，例如專注於遮擋場景、多樣性場景及極端變形場景。相比之下，SA-V 數據集不僅專注於完整物體的分割，還涵蓋了子部件，並且包含大量遮罩。

## 3. 任務：提示式視覺分割

提示式視覺分割（PVS）任務允許用戶在影片的任意幀上向模型提供提示。提示可以是正/負點擊、框選或遮罩，用來定義要分割的物體，或是對模型預測的結果進行修正。為了提供互動體驗，當模型在特定幀上接收到提示時，應立即回應並輸出該物體在該幀上的有效分割遮罩。在接收到初步提示（單個或多個提示）後，模型會將這些提示傳播至整個影片的其他

## 4. 模型

在這段內容中，作者介紹了 **Segment Anything Model 2 (SAM 2)**，這是一個針對影片和影像分割的模型。SAM 2 支援用戶在影片任意幀上通過點擊、框選或遮罩進行提示分割。模型不僅能夠及時回應提示，還能根據過去的預測和提示幀記憶進行處理，從而提升分割效果。

模型應該是使用類似Vision Transformer (ViT)的架構運作的。

**關鍵點包括：**

1. **模型的核心架構**：
   - SAM 2 基於 SAM 模型進行擴展，應用於影片分割中。
   - 該模型利用影像編碼器生成無條件的特徵嵌入，並通過記憶注意力模組來處理當前幀的特徵，考慮過去幀的預測和提示。
   
2. **提示與分割**：
   - 用戶可以使用點擊或框選來提示物體的分割範圍，模型會自動在後續幀中傳播這些提示。
   - 對於不明確的提示，模型會預測多個遮罩，以確保結果的準確性。

3. **記憶模組**：
   - 記憶模組會存儲過去幀的預測，並根據提示進行修正和增強，最終生成高解析度的分割結果。

4. **訓練過程**：
   - 模型通過聯合訓練，同時在影像和影片數據上進行訓練，並模擬用戶互動過程來不斷優化分割效果。

簡單來說，SAM 2 是對 SAM 模型的擴展，專注於影片分割，並利用記憶模組來處理多幀信息，從而提升模型在影片分割任務中的性能。


## 5. 數據

### 數據引擎：

為了構建一個能夠「分割影片中任意物體」的模型，研究人員開發了一個數據引擎來生成大規模且多樣的影片分割數據集。數據生成過程分為三個階段，每個階段都在提升自動化的同時確保數據質量：

1. **第一階段：每幀使用 SAM**  
   這一階段人工標註者在每幀上使用基於影像的 SAM 模型，並通過手動工具（如「畫筆」和「橡皮擦」）進行精確的像素級編輯。這是逐幀處理的方式，平均每幀註解時間較長，但能夠保證每幀高質量的空間標註。此階段共收集了 16,000 個遮罩，來自 1,400 部影片。

2. **第二階段：SAM + SAM 2 Mask**  
   在這一階段，SAM 2 被引入進行遮罩提示。標註者首先生成第一幀的遮罩，然後使用 SAM 2 Mask 將遮罩推廣至其他幀，生成完整的時空遮罩。標註者可手動調整結果，減少了註解時間，並且增強了效率。這一階段共收集了 63,500 個遮罩。

3. **第三階段：SAM 2 完整版**  
   最後階段使用了完整功能的 SAM 2 模型，該模型支援點、框、遮罩等提示類型。SAM 2 還記憶了物體的時空維度，使得標註者僅需在中間幀進行局部修正，極大地提升了效率。註解時間縮短至每幀 4.5 秒，比第一階段快了 8.4 倍。此階段收集了 197,000 個遮罩。

---

### SA-V 資料集：

最終生成的 **SA-V 資料集** 是目前最大且多樣性最豐富的影片分割數據集之一，涵蓋了 50,900 部影片和 642,600 個遮罩。這些影片由來自 47 個國家的眾包工作者拍攝，並包括多樣化的場景和人群（包括年齡和性別）。資料集包含更多遮罩數量，並顯示出比其他現有影片分割數據集更高的多樣性與準確性。

---

### 質量驗證與自動化：

為確保數據質量，研究團隊引入了驗證步驟，通過標註者對生成的遮罩進行檢查。對於質量不佳的遮罩（如無法正確跟蹤物體的遮罩），則進行重新標註。為提高效率，還引入了自動生成遮罩的功能，輔助標註者進行校正，這保證了數據集中遮罩的質量和多樣性。

該數據引擎的三個階段大大提高了數據標註的效率和準確性，而 SA-V 資料集成為未來影片分割研究的重要資源。

## 6. 零樣本實驗

**影片任務**：
- SAM 2 在零樣本影片分割任務中表現出色，尤其是在提示式和半監督式影片物體分割方面。研究團隊對提示式分割進行了離線和在線評估，結果表明 SAM 2 能夠使用較少的點擊數量來生成高質量的分割結果。
- **提示式影片分割** 中，SAM 2 優於其他基準模型（如 SAM+XMem++ 和 SAM+Cutie），能夠在少量提示下生成準確的分割結果，並且在保持結果穩定性方面效果卓越。
- 在 **半監督式影片物體分割** 中，SAM 2 同樣表現出色，特別是在使用點擊或框選提示的情況下，其分割準確度明顯超過其他模型。

**公平性評估**：
- SAM 2 在不同性別和年齡群體中的分割表現有一定的差異，但這些差異可能是由提示的不確定性引起的。模型在年齡段（例如 18-26 歲 vs 50 歲以上）和性別（男性 vs 女性）的表現上仍然相對一致，但提示類型和結果的模糊性導致了一些預測上的偏差。

**影像任務**：
- SAM 2 在影像分割任務中也顯示出卓越的零樣本學習能力。與 SAM 相比，SAM 2 在 23 個零樣本影像數據集上取得了更高的 **mIoU** 準確度，並且速度快了 6 倍。
- 當將 SA-1B 和影片數據混合訓練時，SAM 2 的準確度進一步提高，展示了該模型在不同數據集和任務類型中的廣泛適用性。

SAM 2 展示了在多種零樣本影片和影像分割任務中的優異性能。無論是在提示式分割、半監督分割還是公平性評估方面，SAM 2 都超越了現有的最先進技術，並能夠在更少交互的情況下提供高準確率和高效的分割結果。

## 7. 與半監督式 VOS 最先進技術的比較

在這一章節，作者對比了 **SAM 2** 與其他現有的最先進半監督式 VOS（視覺物體分割）技術，並展現了 SAM 2 的優勢。具體內容如下：

1. **評估影像編碼器的大小影響**：
   - 兩個版本的 SAM 2，分別基於 Hiera-B+ 和 Hiera-L 編碼器，進行了速度與準確性的平衡測試。結果顯示，Hiera-B+ 能夠以 43.8 FPS 的速度實時運行，而 Hiera-L 則運行速度較慢，為 30.2 FPS。
   
2. **與現有技術的比較**：
   - 在多個測試數據集（如 MOSE、DAVIS、LVOS、SA-V 和 YTVOS）中，SAM 2 的性能顯著優於其他技術。特別是在準確性指標（J&F 和 G）上，SAM 2 在表現上全面超越了現有技術，尤其是基於 Hiera-L 的 SAM 2，在準確性和速度之間找到了較佳的平衡點。
   
3. **使用較大影像編碼器的效果**：
   - 結果顯示，使用較大影像編碼器（如 Hiera-L）能夠大幅提升準確度，尤其是在更具挑戰性的數據集上（如 SA-V 和 YTVOS）。SAM 2 在這些數據集上展現了顯著的精度提升，使其成為半監督 VOS 領域中的領先技術之一。

### 第8章 數據與模型消融分析(對比分析)

#### 8.1 數據消融分析(對比分析)
在這部分，作者詳細描述了在不同數據混合下對 SAM 2 模型的準確性進行比較。作者進行了以下實驗：

1. **數據混合實驗**：
   - 模型在 **SA-1B** 和 **SA-V** 上進行預訓練，然後根據不同數據組合進行微調，並針對不同的基準數據集進行測試。
   - 結果表明，將不同數據集混合（如 **VOS**、**MOSE** 和 **SA-V**）可以顯著提高模型在零樣本數據集上的準確性。例如，在某些數據集上的 **J&F** 準確度提高了 **12.1%**。

2. **數據質量與篩選策略**：
   - 試驗中，作者使用了不同的數據子集進行篩選策略測試，包括隨機抽取或基於編輯次數最多的框架進行篩選。結果顯示，選擇編輯過最多的框架能提高模型的性能，但並未超過使用完整數據集的效果。

#### 8.2 模型架構消融分析

1. **容量消融實驗**：
   - 作者通過改變輸入大小（解析度）、幀數、記憶體大小、通道大小等參數，對模型進行了多種容量消融實驗。
   - 結果表明，使用更大的輸入解析度（如 1024x1024）能顯著提升模型在影像任務上的表現，尤其是在 **SA-23** 數據集上的表現，但速度有所下降。
   - 增加記憶體數量對長期物體追蹤的幫助明顯，且調整影像編碼器大小和記憶體大小也能進一步提升性能。

2. **相對位置編碼消融**：
   - 測試結果表明，移除影像編碼器中的相對位置編碼（RPB）並啟用 **FlashAttention-2** 可提高速度，特別是在 1024 輸入解析度下效果顯著。

3. **記憶體架構消融**：
   - 作者進一步探討了如何將記憶特徵餵入 GRU 中。實驗表明，直接存儲記憶特徵並使用物件指針進行交叉注意，能在 **9 個零樣本數據集** 上顯著提高性能。
   - 在某些情況下，儲存記憶特徵的直接方法簡單且高效，並在多數場景下超過了使用 GRU 的效果。

---

通過這些消融實驗，作者確認了模型在不同數據混合下的表現變化，並通過模型架構上的多種優化（如記憶體大小、輸入大小的調整）顯著提升了 SAM 2 在各類影像與影片數據集上的分割準確性與速度。

### 結論

作者在本文中呈現了 **Segment Anything** 在影像領域進化到影片領域的自然過程，基於三個關鍵要素：(i) 將可提示的分割任務擴展到影片，(ii) 將 **SAM** 架構應用於影片時，讓其具備使用記憶體的能力，(iii) 使用多樣化的 **SA-V** 數據集作為影片分割的訓練和基準測試。作者相信 **SAM 2** 在視覺感知領域標誌著一次顯著的進步，並將自身的貢獻定位為推動該領域進一步研究和應用的里程碑。
