# Calculus.md

## 目錄
1. 前言
2. 基本概念回顧
    - 實數函數與連續性
    - 導數 (Derivative)
    - 偏導數 (Partial Derivative)
    - 梯度 (Gradient)
3. 多變數微分與向量微分算子
    - 偏微分的定義與計算
    - 梯度、Hessian 與高階偏導數
4. 鏈式法則 (Chain Rule)
    - 單變數鏈式法則
    - 多變數鏈式法則
    - 在神經網路中計算梯度與反向傳播的核心原理
5. 向量與矩陣微分
    - 對向量/矩陣函數求導的基本規則
    - 常見的向量與矩陣微分公式速查
    - Matrix Cookbook 作為參考資源
6. 深度學習中的應用
    - 損失函數對權重、偏置的梯度求解
    - 反向傳播 (Backpropagation) 演算法的數學推導
    - 自動微分 (Automatic Differentiation) 的概念
7. 最優化與學習率選擇
    - 梯度下降 (Gradient Descent) 與 Learning Rate 的角色
    - 基於梯度的優化方法：動量 (Momentum)、Adam、RMSProp
8. 數值穩定性與技巧
    - 避免浮點下溢/上溢的計算方法（如使用 Log-Sum-Exp）
    - 函數在邊界處的微分特性與數值行為
9. 延伸閱讀與參考資源

---

## 1. 前言

微積分是機器學習與深度學習的基石之一。在神經網路訓練中，我們需要透過優化演算法對網路參數 (weights, biases) 進行調整，而這些調整的方向與幅度端賴對「目標函數 (如損失函數)」的梯度資訊。

本章將從基礎導數、偏導數、梯度開始講起，介紹計算多變數函數導數的基本概念，並深入探討鏈式法則及其在神經網路反向傳播中的核心地位。我們也將討論自動微分工具的概念，以協助在實作上快速且精確地取得梯度。

## 2. 基本概念回顧

- **導數 (Derivative)**：對單變數函數 f(x)，f'(x) 表示 x 點處的瞬時變化率。  
  幾何詮釋為曲線在該點的切線斜率。

- **偏導數 (Partial Derivative)**：對多變數函數 f(x_1, x_2, ..., x_n)，固定其他變數不變，僅對某一變數求導。如  
  ∂f/∂x_1 表示當僅 x_1 微小變動時，f 的變化率。

- **梯度 (Gradient)**：將 f 對各變數的偏導數組合成向量，即 ∇f(x) = (∂f/∂x_1, ∂f/∂x_2, ..., ∂f/∂x_n)。  
  梯度指出 f 增長最快的方向，而 −∇f 則為最快下降方向，在梯度下降中非常重要。

## 3. 多變數微分與向量微分算子

對於多變數函數，我們考慮：

- **Hessian 矩陣**：包含所有二階偏導數的矩陣，用於分析函數曲率 (Curvature)。

- 對高維度輸入，微分提供有用的線索來了解函數在局部的性質。機器學習中，理解 Hessian 有助於分析代價函數的優化性質和收斂行為。

## 4. 鏈式法則 (Chain Rule)

- **單變數鏈式法則**：若 y = f(u) 且 u = g(x)，則 dy/dx = f'(u)*g'(x)。
  
- **多變數鏈式法則**：若 y = f(u_1, u_2, ..., u_m)，而每個 u_i 又是其他變數的函數 u_i(x_1, ..., x_n)，則  
  ∂y/∂x_j = Σ_i (∂f/∂u_i)(∂u_i/∂x_j)。

- 在神經網路中，輸入經過多層線性/非線性變換，鏈式法則可將輸入到輸出間的偏導數拆解成每層的局部導數相乘，進而推導出反向傳播演算法。

## 5. 向量與矩陣微分

- **向量函數對向量求導**：常見於線性代數與機器學習的損失函數，如 L2 損失 L = ∥Ax - b∥²。求解 ∂L/∂x 時需用到向量/矩陣微分法則。

- 常用結果範例：  
  - 若 f(x) = a^T x，則 ∇_x f(x) = a。  
  - 若 f(x) = x^T A x (A 對稱)，則 ∇_x f(x) = (A + A^T)x = 2Ax。  
  - 若 f(X) = Tr(A^T X)，則 ∇_X f(X) = A。

- **Matrix Cookbook** 提供大量向量/矩陣微分的公式，是實務中常參考的資源。

## 6. 深度學習中的應用

- **損失函數對參數的梯度計算**：訓練神經網路時，我們必須計算權重 W 與偏置 b 對損失 L 的梯度，然後更新 W、b。

- **反向傳播 (Backpropagation)**：由輸出層開始，往回計算梯度的一種高效算法。透過鏈式法則，可將整個網路梯度計算分解為多個「局部偏導數」的連乘，極大化效率。

- **自動微分 (Automatic Differentiation)**：數值工具（例如 TensorFlow、PyTorch）可自動生成梯度，而無需手動推導公式。自動微分本質上是利用鏈式法則的程式實作。

## 7. 最優化與學習率選擇

- 利用梯度資訊，我們可以進行梯度下降 (Gradient Descent) 來最小化損失函數。

- **學習率 (Learning Rate)**：決定每次更新步伐大小。若步伐過大，可能不穩定；太小，則收斂太慢。

- 不同優化器（如 Momentum、Adam、RMSProp）運用梯度的歷史資訊或對梯度做平滑處理，提升訓練速度與穩定性。

## 8. 數值穩定性與技巧

- 在實務上計算梯度時，可能面臨指數函數的上溢/下溢等問題。例如計算 softmax 時，常須對輸入做 shift 處理以防數值不穩定。

- 對邏輯函數 (sigmoid) 或對數概率 (log-probability) 計算梯度時，若直接使用原生函數可能會發生數值下溢。透過數值穩定的技巧 (如使用 log-sum-exp 技巧) 可避免此類問題。

## 9. 延伸閱讀與參考資源

- **建議閱讀**：  
  - “Deep Learning” (Goodfellow, Bengio, Courville) 中的數學基礎章節  
  - 《Matrix Cookbook》：對向量與矩陣的微分規則作詳細整理  
  - 有關自動微分的文獻與框架文件（TensorFlow、PyTorch 官方文件）

